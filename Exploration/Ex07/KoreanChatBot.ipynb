{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10-1. 프로젝트: 한국어 데이터로 챗봇 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1 & 2 데이터 수집하기 & 데이터 전처리하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('ChatbotData.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 중복인 열의 데이터를 확인하기\n",
    "duplicate_indices = []\n",
    "\n",
    "unique_Q = data['Q'].unique()\n",
    "\n",
    "for unique in unique_Q:\n",
    "    indices = data[data['Q'] == unique].index.tolist()\n",
    "    if len(indices) > 1:\n",
    "        print(f\"중복된 헤드라인: {unique}\")\n",
    "        print(f\"중복된 행의 인덱스: {indices}\")\n",
    "        duplicate_indices.extend(indices)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_data = data.loc[duplicate_indices]\n",
    "duplicate_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 질문이 겹치는 경우, 항상 답변이 일치하지는 않는 것을 확인\n",
    "# => 답변을 기준으로 drop (같은 질문도 다르게 답변할 수 있도록)\n",
    "data.drop_duplicates(subset = ['A'], inplace=True)\n",
    "print('전체 샘플수 :', (len(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "한국어와 영어 전처리 과정의 차이(https://haru0229.tistory.com/57)\n",
    "\n",
    "한국어는 영어와 달리 띄어쓰기로 구분하기 어렵다 (의존 형태소 떄문) => 띄어쓰기로 토큰화하는게 아니라 형태소 토큰화를 수행해야함 \n",
    "+ 전처리하면서 생긴 문제 및 해결방안\n",
    "1. 형태소 토큰화 수행\n",
    "2. 의존 형태소를 불용어 처리 (Q(질문)에만 적용 , A(답변)은 자연스럽게 출력될 수 있도록 불용어 처리X) => 처음 프로젝트를 시작할 때에는 불용어 처리를 고려했지만, Q,A 모두 문장 길이가 짧아 불용어 처리를 하지 않아도 좋은 성능을 보여 최종 제출은 불용어 처리를 하지 않았다.\n",
    "3. 정규화 사전은 찾지 못함 (정규화하지 않으면 vocab 크기가 커지는 문제 뿐만 아니라 의미 있는 단어가 자주 등장하지 않는 토큰으로 처리되어 삭제될 수 있음 => 지금은 정규화 없이 진행 후 추후에 다시 고민)\n",
    "4. 영어가 나오면 우선 한글로 바꿈\n",
    "5. 숫자의 경우 한글로 바꾸지는 않음\n",
    "6. 3~5에서 발생하는 문제의 핵심은 결국 나중에 학습을 위해 드문 토큰들 삭제할 때, 지금 고민했던 토큰들이 삭제가 되는 것이 문제임, 우선 vocab크기가 너무 크지 않다면 따로 삭제하지 않는 것으로 진행 => 뒤에 전처리 요구사항에 SubwordTextEncoder를 사용하기 때문에 결국 어느정도 삭제될 수 밖에 없었음 (vocab size를 더 키우면 더 많이 보존할 수 있음; 회고에 추가 내용이 있습니다) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def english_to_korean(word):\n",
    "    \"\"\"\n",
    "    영어 단어를 한글 발음으로 변환\n",
    "    예) SSD -> 에스에스디\n",
    "    \"\"\"\n",
    "    mapping = {\n",
    "        'a': '에이', 'b': '비', 'c': '씨', 'd': '디', 'e': '이',\n",
    "        'f': '에프', 'g': '지', 'h': '에이치', 'i': '아이',\n",
    "        'j': '제이', 'k': '케이', 'l': '엘', 'm': '엠',\n",
    "        'n': '엔', 'o': '오', 'p': '피', 'q': '큐',\n",
    "        'r': '알', 's': '에스', 't': '티', 'u': '유', \n",
    "        'v': '브이', 'w': '더블유', 'x': '엑스', 'y': '와이', 'z': '제트'\n",
    "    }\n",
    "    return \"\".join(mapping.get(ch.lower(), ch) for ch in word)\n",
    "\n",
    "def preprocess_korean_sentence(sentence, remove_stopwords=True):\n",
    "    \"\"\"\n",
    "    1. 앞뒤 공백 제거\n",
    "    2. 영어 단어는 한글 발음으로 변환 (예: SSD -> 에스에스디)\n",
    "    3. 최종적으로 한글 숫자 이외의 문자들을 제거\n",
    "    \"\"\"\n",
    "    # 1. 앞뒤 공백 제거\n",
    "    sentence = sentence.strip()\n",
    "       \n",
    "    # 2. 영어 변환: [A-Za-z]+ 패턴에 대해 영어 단어를 한글 발음으로 치환\n",
    "    sentence = re.sub(r'[A-Za-z]+', lambda m: english_to_korean(m.group()), sentence)\n",
    "    \n",
    "    # 3. 한글, 숫자, 공백을 제외한 문자 제거\n",
    "    sentence = re.sub(r'[^가-힣0-9\\s]', '', sentence)\n",
    "    \n",
    "    \n",
    "    return sentence\n",
    "\n",
    "# 예시 사용\n",
    "example_text = \"SSD 제품은 12개의 기능이 있습니다. 가격은 1500원.\"\n",
    "print(preprocess_korean_sentence(example_text, remove_stopwords=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.reset_index(inplace=True, drop=True)\n",
    "\n",
    "preprocess_korean_sentence(data.loc[3,'Q'])\n",
    "#영어가 한국어로 잘 변환되는 것을 확인, 현재 데이터셋에서 대부분 영어는 약어로 쓰이기 때문에\n",
    "#이렇게 전처리하는게 적절하다고 판단"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data)):\n",
    "   data.loc[i,'question'] = preprocess_korean_sentence(data.loc[i,'Q'])\n",
    "   data.loc[i,'answer'] = preprocess_korean_sentence(data.loc[i,'A'])\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3. SubwordTextEncoder 사용하기\n",
    "\n",
    "한국어 데이터는 형태소 분석기를 사용하여 토크나이징을 해야 한다고 많은 분이 알고 있습니다. 하지만 여기서는 형태소 분석기가 아닌 위 실습에서 사용했던 내부 단어 토크나이저인 SubwordTextEncoder를 그대로 사용해보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = data['question']\n",
    "answers = data['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "print(\"살짝 오래 걸릴 수 있어요. 스트레칭 한 번 해볼까요? 👐\")\n",
    "\n",
    "# 질문과 답변 데이터셋에 대해서 Vocabulary 생성\n",
    "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(questions + answers, target_vocab_size=2**13)\n",
    "print(\"슝=3 \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시작 토큰과 종료 토큰에 고유한 정수를 부여합니다.\n",
    "START_TOKEN, END_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size + 1]\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('START_TOKEN의 번호 :' ,[tokenizer.vocab_size])\n",
    "print('END_TOKEN의 번호 :' ,[tokenizer.vocab_size + 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시작 토큰과 종료 토큰을 고려하여 +2를 하여 단어장의 크기를 산정합니다.\n",
    "VOCAB_SIZE = tokenizer.vocab_size + 2\n",
    "print(VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임의의 22번째 샘플에 대해서 정수 인코딩 작업을 수행.\n",
    "# 각 토큰을 고유한 정수로 변환\n",
    "print('정수 인코딩 후의 21번째 질문 샘플: {}'.format(tokenizer.encode(questions[21])))\n",
    "print('정수 인코딩 후의 21번째 답변 샘플: {}'.format(tokenizer.encode(answers[21])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 질문과 답변의 토큰 길이 구하기\n",
    "question_lengths = [len(tokenizer.encode(q)) for q in questions]\n",
    "answer_lengths = [len(tokenizer.encode(a)) for a in answers]\n",
    "\n",
    "# 전체 길이 리스트\n",
    "all_lengths = question_lengths + answer_lengths\n",
    "\n",
    "# 길이 분포 시각화\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.hist(all_lengths, bins=50, alpha=0.7, color='skyblue', label='Lengths')\n",
    "plt.axvline(x=np.percentile(all_lengths, 97), color='b', linestyle='--', label='97 Percentile')\n",
    "plt.axvline(x=np.percentile(all_lengths, 99), color='r', linestyle='--', label='99 Percentile')\n",
    "\n",
    "plt.xlabel('Sequence Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Sequence Length Distribution')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# 통계 분석\n",
    "print(f\"최대 길이: {np.max(all_lengths)}\")\n",
    "print(f\"평균 길이: {np.mean(all_lengths):.2f}\")\n",
    "print(f\"97% 백분위수: {np.percentile(all_lengths, 97)}\")\n",
    "print(f\"99% 백분위수: {np.percentile(all_lengths, 99)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 샘플 99%를 활용하기 위해 최대 길이를 사용\n",
    "MAX_LENGTH = 13\n",
    "\n",
    "# 정수 인코딩, 최대 길이를 초과하는 샘플 제거, 패딩\n",
    "def tokenize_and_filter(inputs, outputs):\n",
    "  tokenized_inputs, tokenized_outputs = [], []\n",
    "  \n",
    "  for (sentence1, sentence2) in zip(inputs, outputs):\n",
    "    # 정수 인코딩 과정에서 시작 토큰과 종료 토큰을 추가\n",
    "    sentence1 = START_TOKEN + tokenizer.encode(sentence1) + END_TOKEN\n",
    "    sentence2 = START_TOKEN + tokenizer.encode(sentence2) + END_TOKEN\n",
    "\n",
    "    # 최대 길이 40 이하인 경우에만 데이터셋으로 허용\n",
    "    if len(sentence1) <= MAX_LENGTH and len(sentence2) <= MAX_LENGTH:\n",
    "      tokenized_inputs.append(sentence1)\n",
    "      tokenized_outputs.append(sentence2)\n",
    "  \n",
    "  # 최대 길이 13으로 모든 데이터셋을 패딩\n",
    "  tokenized_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "      tokenized_inputs, maxlen=MAX_LENGTH, padding='post')\n",
    "  tokenized_outputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "      tokenized_outputs, maxlen=MAX_LENGTH, padding='post')\n",
    "  \n",
    "  return tokenized_inputs, tokenized_outputs\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions, answers = tokenize_and_filter(questions, answers)\n",
    "print('단어장의 크기 :',(VOCAB_SIZE))\n",
    "print('필터링 후의 질문 샘플 개수: {}'.format(len(questions)))\n",
    "print('필터링 후의 답변 샘플 개수: {}'.format(len(answers)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 20000\n",
    "\n",
    "# 디코더는 이전의 target을 다음의 input으로 사용합니다.\n",
    "# 이에 따라 outputs에서는 START_TOKEN을 제거하겠습니다.\n",
    "dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {\n",
    "        'inputs': questions,\n",
    "        'dec_inputs': answers[:, :-1]\n",
    "    },\n",
    "    {\n",
    "        'outputs': answers[:, 1:]\n",
    "    },\n",
    "))\n",
    "\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4. 모델 구성하기\n",
    "위 실습 내용을 참고하여 트랜스포머 모델을 구현합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 포지셔널 인코딩 레이어\n",
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "\n",
    "  def __init__(self, position, d_model):\n",
    "    super(PositionalEncoding, self).__init__()\n",
    "    self.pos_encoding = self.positional_encoding(position, d_model)\n",
    "\n",
    "  def get_angles(self, position, i, d_model):\n",
    "    angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
    "    return position * angles\n",
    "\n",
    "  def positional_encoding(self, position, d_model):\n",
    "    # 각도 배열 생성\n",
    "    angle_rads = self.get_angles(\n",
    "        position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n",
    "        i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],\n",
    "        d_model=d_model)\n",
    "\n",
    "    # 배열의 짝수 인덱스에는 sin 함수 적용\n",
    "    sines = tf.math.sin(angle_rads[:, 0::2])\n",
    "    # 배열의 홀수 인덱스에는 cosine 함수 적용\n",
    "    cosines = tf.math.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    # sin과 cosine이 교차되도록 재배열\n",
    "    pos_encoding = tf.stack([sines, cosines], axis=0)\n",
    "    pos_encoding = tf.transpose(pos_encoding,[1, 2, 0]) \n",
    "    pos_encoding = tf.reshape(pos_encoding, [position, d_model])\n",
    "\n",
    "    pos_encoding = pos_encoding[tf.newaxis, ...]\n",
    "    return tf.cast(pos_encoding, tf.float32)\n",
    "\n",
    "  def call(self, inputs):\n",
    "    return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 스케일드 닷 프로덕트 어텐션 함수\n",
    "def scaled_dot_product_attention(query, key, value, mask):\n",
    "  # 어텐션 가중치는 Q와 K의 닷 프로덕트\n",
    "  matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
    "\n",
    "  # 가중치를 정규화\n",
    "  depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "  logits = matmul_qk / tf.math.sqrt(depth)\n",
    "\n",
    "  # 패딩에 마스크 추가\n",
    "  if mask is not None:\n",
    "    logits += (mask * -1e9)\n",
    "\n",
    "  # softmax적용\n",
    "  attention_weights = tf.nn.softmax(logits, axis=-1)\n",
    "\n",
    "  # 최종 어텐션은 가중치와 V의 닷 프로덕트\n",
    "  output = tf.matmul(attention_weights, value)\n",
    "  return output\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "\n",
    "  def __init__(self, d_model, num_heads, name=\"multi_head_attention\"):\n",
    "    super(MultiHeadAttention, self).__init__(name=name)\n",
    "    self.num_heads = num_heads\n",
    "    self.d_model = d_model\n",
    "\n",
    "    assert d_model % self.num_heads == 0 # 나누어 떨어지지 않으면 수행 불가\n",
    "\n",
    "    self.depth = d_model // self.num_heads # 여기가 Q,K,V가 muti-head로 나뉘었을 때의 차원 수\n",
    "\n",
    "    self.query_dense = tf.keras.layers.Dense(units=d_model)\n",
    "    self.key_dense = tf.keras.layers.Dense(units=d_model)\n",
    "    self.value_dense = tf.keras.layers.Dense(units=d_model) # 아직 어텐션 계산 전 형태\n",
    "\n",
    "    self.dense = tf.keras.layers.Dense(units=d_model)# 다음 레이어로 넘어가는 최종 레이어 \n",
    "\n",
    "  def split_heads(self, inputs, batch_size):\n",
    "    inputs = tf.reshape(\n",
    "        inputs, shape=(batch_size, -1, self.num_heads, self.depth))# -1은 입력 문장의 길이\n",
    "    return tf.transpose(inputs, perm=[0, 2, 1, 3])# (batch_size, num_heads, seq_len, depth)\n",
    "\n",
    "  def call(self, inputs):\n",
    "    query, key, value, mask = inputs['query'], inputs['key'], inputs[\n",
    "        'value'], inputs['mask']\n",
    "    batch_size = tf.shape(query)[0]\n",
    "\n",
    "    # Q, K, V에 각각 Dense를 적용합니다\n",
    "    query = self.query_dense(query)\n",
    "    key = self.key_dense(key)\n",
    "    value = self.value_dense(value)\n",
    "\n",
    "    # 병렬 연산을 위한 머리를 여러 개 만듭니다\n",
    "    query = self.split_heads(query, tf.shape(query)[0])\n",
    "    key = self.split_heads(key, tf.shape(key)[0])\n",
    "    value = self.split_heads(value, tf.shape(value)[0])\n",
    "\n",
    "    # 스케일드 닷 프로덕트 어텐션 함수\n",
    "    scaled_attention = scaled_dot_product_attention(query, key, value, mask)\n",
    "\n",
    "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "\n",
    "    # 어텐션 연산 후에 각 결과를 다시 연결(concatenate)합니다\n",
    "    concat_attention = tf.reshape(scaled_attention,\n",
    "                                  (batch_size, -1, self.d_model))# (batch_size, seq_len, d_model)\n",
    "\n",
    "    # 최종 결과에도 Dense를 한 번 더 적용합니다\n",
    "    outputs = self.dense(concat_attention)\n",
    "\n",
    "    return outputs\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(x):\n",
    "  mask = tf.cast(tf.math.equal(x, 0), tf.float32)\n",
    "  # (batch_size, 1, 1, sequence length)\n",
    "  return mask[:, tf.newaxis, tf.newaxis , : ]\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(x):\n",
    "  seq_len = tf.shape(x)[1]\n",
    "  look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "  padding_mask = create_padding_mask(x)\n",
    "  return tf.maximum(look_ahead_mask, padding_mask)\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인코더 하나의 레이어를 함수로 구현.\n",
    "# 이 하나의 레이어 안에는 두 개의 서브 레이어가 존재합니다.\n",
    "def encoder_layer(units, d_model, num_heads, dropout, name=\"encoder_layer\"):\n",
    "  inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "\n",
    "  # 패딩 마스크 사용\n",
    "  padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "\n",
    "  # 첫 번째 서브 레이어 : 멀티 헤드 어텐션 수행 (셀프 어텐션)\n",
    "  attention = MultiHeadAttention(\n",
    "      d_model, num_heads, name=\"attention\")({\n",
    "          'query': inputs,\n",
    "          'key': inputs,\n",
    "          'value': inputs,\n",
    "          'mask': padding_mask\n",
    "      })\n",
    "\n",
    "  # 어텐션의 결과는 Dropout과 Layer Normalization이라는 훈련을 돕는 테크닉을 수행\n",
    "  attention = tf.keras.layers.Dropout(rate=dropout)(attention)\n",
    "  attention = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(inputs + attention)\n",
    "\n",
    "  # 두 번째 서브 레이어 : 2개의 완전연결층\n",
    "  outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention)\n",
    "  outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "\n",
    "  # 완전연결층의 결과는 Dropout과 LayerNormalization이라는 훈련을 돕는 테크닉을 수행\n",
    "  outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "  outputs = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(attention + outputs)\n",
    "\n",
    "  return tf.keras.Model(\n",
    "      inputs=[inputs, padding_mask], outputs=outputs, name=name)\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(vocab_size,\n",
    "            num_layers,\n",
    "            units,\n",
    "            d_model,\n",
    "            num_heads,\n",
    "            dropout,\n",
    "            name=\"encoder\"):\n",
    "  inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "\n",
    "  # 패딩 마스크 사용\n",
    "  padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "\n",
    "  # 임베딩 레이어\n",
    "  embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "  embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "\n",
    "  # 포지셔널 인코딩\n",
    "  embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
    "\n",
    "  outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "\n",
    "  # num_layers만큼 쌓아올린 인코더의 층.\n",
    "  for i in range(num_layers):\n",
    "    outputs = encoder_layer(\n",
    "        units=units,\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        dropout=dropout,\n",
    "        name=\"encoder_layer_{}\".format(i),\n",
    "    )([outputs, padding_mask])\n",
    "\n",
    "  return tf.keras.Model(\n",
    "      inputs=[inputs, padding_mask], outputs=outputs, name=name)\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 디코더 하나의 레이어를 함수로 구현.\n",
    "# 이 하나의 레이어 안에는 세 개의 서브 레이어가 존재합니다.\n",
    "def decoder_layer(units, d_model, num_heads, dropout, name=\"decoder_layer\"):\n",
    "  inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "  enc_outputs = tf.keras.Input(shape=(None, d_model), name=\"encoder_outputs\")\n",
    "  look_ahead_mask = tf.keras.Input(\n",
    "      shape=(1, None, None), name=\"look_ahead_mask\")\n",
    "  padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
    "\n",
    "  # 첫 번째 서브 레이어 : 멀티 헤드 어텐션 수행 (셀프 어텐션)\n",
    "  attention1 = MultiHeadAttention(\n",
    "      d_model, num_heads, name=\"attention_1\")(inputs={\n",
    "          'query': inputs,\n",
    "          'key': inputs,\n",
    "          'value': inputs,\n",
    "          'mask': look_ahead_mask\n",
    "      })\n",
    "\n",
    "  # 멀티 헤드 어텐션의 결과는 LayerNormalization이라는 훈련을 돕는 테크닉을 수행\n",
    "  attention1 = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(attention1 + inputs)\n",
    "\n",
    "  # 두 번째 서브 레이어 : 마스크드 멀티 헤드 어텐션 수행 (인코더-디코더 어텐션)\n",
    "  attention2 = MultiHeadAttention(\n",
    "      d_model, num_heads, name=\"attention_2\")(inputs={\n",
    "          'query': attention1,\n",
    "          'key': enc_outputs,\n",
    "          'value': enc_outputs,\n",
    "          'mask': padding_mask\n",
    "      })\n",
    "\n",
    "  # 마스크드 멀티 헤드 어텐션의 결과는\n",
    "  # Dropout과 LayerNormalization이라는 훈련을 돕는 테크닉을 수행\n",
    "  attention2 = tf.keras.layers.Dropout(rate=dropout)(attention2)\n",
    "  attention2 = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(attention2 + attention1)\n",
    "\n",
    "  # 세 번째 서브 레이어 : 2개의 완전연결층\n",
    "  outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention2)\n",
    "  outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "\n",
    "  # 완전연결층의 결과는 Dropout과 LayerNormalization 수행\n",
    "  outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "  outputs = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(outputs + attention2)\n",
    "\n",
    "  return tf.keras.Model(\n",
    "      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
    "      outputs=outputs,\n",
    "      name=name)\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder(vocab_size,\n",
    "            num_layers,\n",
    "            units,\n",
    "            d_model,\n",
    "            num_heads,\n",
    "            dropout,\n",
    "            name='decoder'):\n",
    "  inputs = tf.keras.Input(shape=(None,), name='inputs')\n",
    "  enc_outputs = tf.keras.Input(shape=(None, d_model), name='encoder_outputs')\n",
    "  look_ahead_mask = tf.keras.Input(\n",
    "      shape=(1, None, None), name='look_ahead_mask')\n",
    "\n",
    "  # 패딩 마스크\n",
    "  padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
    "  \n",
    "  # 임베딩 레이어\n",
    "  embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "  embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "\n",
    "  # 포지셔널 인코딩\n",
    "  embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
    "\n",
    "  # Dropout이라는 훈련을 돕는 테크닉을 수행\n",
    "  outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "\n",
    "  for i in range(num_layers):\n",
    "    outputs = decoder_layer(\n",
    "        units=units,\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        dropout=dropout,\n",
    "        name='decoder_layer_{}'.format(i),\n",
    "    )(inputs=[outputs, enc_outputs, look_ahead_mask, padding_mask])\n",
    "\n",
    "  return tf.keras.Model(\n",
    "      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
    "      outputs=outputs,\n",
    "      name=name)\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer(vocab_size,\n",
    "                num_layers,\n",
    "                units,\n",
    "                d_model,\n",
    "                num_heads,\n",
    "                dropout,\n",
    "                name=\"transformer\"):\n",
    "  inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "  dec_inputs = tf.keras.Input(shape=(None,), name=\"dec_inputs\")\n",
    "\n",
    "  # 인코더에서 패딩을 위한 마스크\n",
    "  enc_padding_mask = tf.keras.layers.Lambda(\n",
    "      create_padding_mask, output_shape=(1, 1, None),\n",
    "      name='enc_padding_mask')(inputs)\n",
    "\n",
    "  # 디코더에서 미래의 토큰을 마스크 하기 위해서 사용합니다.\n",
    "  # 내부적으로 패딩 마스크도 포함되어져 있습니다.\n",
    "  look_ahead_mask = tf.keras.layers.Lambda(\n",
    "      create_look_ahead_mask,\n",
    "      output_shape=(1, None, None),\n",
    "      name='look_ahead_mask')(dec_inputs)\n",
    "\n",
    "  # 두 번째 어텐션 블록에서 인코더의 벡터들을 마스킹\n",
    "  # 디코더에서 패딩을 위한 마스크\n",
    "  dec_padding_mask = tf.keras.layers.Lambda(\n",
    "      create_padding_mask, output_shape=(1, 1, None),\n",
    "      name='dec_padding_mask')(inputs)\n",
    "\n",
    "  # 인코더\n",
    "  enc_outputs = encoder(\n",
    "      vocab_size=vocab_size,\n",
    "      num_layers=num_layers,\n",
    "      units=units,\n",
    "      d_model=d_model,\n",
    "      num_heads=num_heads,\n",
    "      dropout=dropout,\n",
    "  )(inputs=[inputs, enc_padding_mask])\n",
    "\n",
    "  # 디코더\n",
    "  dec_outputs = decoder(\n",
    "      vocab_size=vocab_size,\n",
    "      num_layers=num_layers,\n",
    "      units=units,\n",
    "      d_model=d_model,\n",
    "      num_heads=num_heads,\n",
    "      dropout=dropout,\n",
    "  )(inputs=[dec_inputs, enc_outputs, look_ahead_mask, dec_padding_mask])\n",
    "\n",
    "  # 완전연결층\n",
    "  outputs = tf.keras.layers.Dense(units=vocab_size, name=\"outputs\")(dec_outputs)\n",
    "\n",
    "  return tf.keras.Model(inputs=[inputs, dec_inputs], outputs=outputs, name=name)\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# 하이퍼파라미터\n",
    "NUM_LAYERS = 2 # 인코더와 디코더의 층의 개수\n",
    "D_MODEL = 256 # 인코더와 디코더 내부의 입, 출력의 고정 차원\n",
    "NUM_HEADS = 8 # 멀티 헤드 어텐션에서의 헤드 수 \n",
    "UNITS = 512 # 피드 포워드 신경망의 은닉층의 크기\n",
    "DROPOUT = 0.1 # 드롭아웃의 비율\n",
    "\n",
    "model = transformer(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    units=UNITS,\n",
    "    d_model=D_MODEL,\n",
    "    num_heads=NUM_HEADS,\n",
    "    dropout=DROPOUT)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(y_true, y_pred):\n",
    "  y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "  \n",
    "  loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "      from_logits=True, reduction='none')(y_true, y_pred)\n",
    "\n",
    "  mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n",
    "  loss = tf.multiply(loss, mask)\n",
    "\n",
    "  return tf.reduce_mean(loss)\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "\n",
    "  def __init__(self, d_model, warmup_steps=4000):\n",
    "    super(CustomSchedule, self).__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "    self.warmup_steps = warmup_steps\n",
    "\n",
    "  def __call__(self, step):\n",
    "    step = tf.cast(step, tf.float32)\n",
    "    arg1 = tf.math.rsqrt(step)\n",
    "    arg2 = step * (self.warmup_steps**-1.5)\n",
    "\n",
    "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = CustomSchedule(D_MODEL)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "  y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "  return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#지금 검증 세트를 따로 구성하지 않았고 생성모델이므로\n",
    "#accuracy가 아닌 BLEU 점수를 기준으로 과적합을 방지\n",
    "# BLEU점수 계산에 inference코드를 활용\n",
    "def decoder_inference(sentence):\n",
    "  sentence = preprocess_korean_sentence(sentence)\n",
    "\n",
    "  # 입력된 문장을 정수 인코딩 후, 시작 토큰과 종료 토큰을 앞뒤로 추가.\n",
    "  # ex) Where have you been? → [[8331   86   30    5 1059    7 8332]]\n",
    "  sentence = tf.expand_dims(\n",
    "      START_TOKEN + tokenizer.encode(sentence) + END_TOKEN, axis=0)\n",
    "\n",
    "  # 디코더의 현재까지의 예측한 출력 시퀀스가 지속적으로 저장되는 변수.\n",
    "  # 처음에는 예측한 내용이 없음으로 시작 토큰만 별도 저장. ex) 8331\n",
    "  output_sequence = tf.expand_dims(START_TOKEN, 0)\n",
    "\n",
    "  # 디코더의 인퍼런스 단계\n",
    "  for i in range(MAX_LENGTH):\n",
    "    # 디코더는 최대 MAX_LENGTH의 길이만큼 다음 단어 예측을 반복합니다.\n",
    "    predictions = model(inputs=[sentence, output_sequence], training=False)\n",
    "    predictions = predictions[:, -1:, :]\n",
    "\n",
    "    # 현재 예측한 단어의 정수\n",
    "    predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "\n",
    "    # 만약 현재 예측한 단어가 종료 토큰이라면 for문을 종료\n",
    "    if tf.equal(predicted_id, END_TOKEN[0]):\n",
    "      break\n",
    "\n",
    "    # 예측한 단어들은 지속적으로 output_sequence에 추가됩니다.\n",
    "    # 이 output_sequence는 다시 디코더의 입력이 됩니다.\n",
    "    output_sequence = tf.concat([output_sequence, predicted_id], axis=-1)\n",
    "\n",
    "  return tf.squeeze(output_sequence, axis=0)\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_generation(sentence):\n",
    "  # 입력 문장에 대해서 디코더를 동작 시켜 예측된 정수 시퀀스를 리턴받습니다.\n",
    "  prediction = decoder_inference(sentence)\n",
    "\n",
    "  # 정수 시퀀스를 다시 텍스트 시퀀스로 변환합니다.\n",
    "  predicted_sentence = tokenizer.decode(\n",
    "      [i for i in prediction if i < tokenizer.vocab_size])\n",
    "\n",
    "  return predicted_sentence\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "# BLEU 점수를 기준으로 조기 종료하는 콜백 클래스\n",
    "class BLEUEarlyStopping(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, sample_questions, sample_answers, patience=5, threshold=0.9, check_interval=10):\n",
    "        \"\"\"\n",
    "        sample_questions, sample_answers: 평가용으로 사용할 원본 질문과 정답 텍스트 (리스트)\n",
    "        patience: BLEU 점수가 개선되지 않는 에포크 수\n",
    "        threshold: BLEU 점수가 이 값 이상이면 조기 종료\n",
    "        \"\"\"\n",
    "        super(BLEUEarlyStopping, self).__init__()\n",
    "        self.sample_questions = sample_questions\n",
    "        self.sample_answers = sample_answers\n",
    "        self.patience = patience\n",
    "        self.threshold = threshold\n",
    "        self.check_interval = check_interval  # 10개 epoch마다 체크\n",
    "        self.best_bleu = -1\n",
    "        self.wait = 0\n",
    "        self.last_bleu = -1  # 마지막 BLEU 점수 저장\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # 매 'check_interval'번째 epoch에 대해 BLEU 점수를 계산\n",
    "        if (epoch + 1) % self.check_interval == 0:\n",
    "            bleu_scores = []\n",
    "            # 준비한 샘플 각각에 대해 예측을 수행하고 BLEU 점수를 계산합니다.\n",
    "            for question, reference in zip(self.sample_questions, self.sample_answers):\n",
    "                reference_decoded = tokenizer.decode([i for i in reference if i < tokenizer.vocab_size]) #답변\n",
    "                question_decoded = tokenizer.decode([i for i in question if i < tokenizer.vocab_size]) #질문\n",
    "\n",
    "                # sentence_generation 함수는 내부에서 decoder_inference를 호출하여 예측된 문장을 문자열로 리턴\n",
    "                predicted = sentence_generation(question_decoded) #질문에 대한 모델의 답변\n",
    "                \n",
    "                # BLEU 점수를 계산하기 위해, 참조와 예측 문장을 단어 단위로 분리\n",
    "                ref_tokens = reference_decoded.split()  # reference는 이제 확실히 문자열\n",
    "                pred_tokens = predicted.split()  # predicted는 이미 문자열\n",
    "\n",
    "                print(\"\\n질문 : \", question_decoded)\n",
    "                print(\"원래 답변 : \", ref_tokens)\n",
    "                print(\"생성 답번 : \", pred_tokens)\n",
    "                \n",
    "                # nltk의 sentence_bleu 함수에 참조는 리스트의 리스트로 전달합니다.\n",
    "                score = sentence_bleu([ref_tokens], pred_tokens, weights=(1, 1, 1, 1)) # 출력 문장의 길이가 짧아 1-gram 기준으로 계산 => 1~4gram 모두 평가하기로 다시 수정\n",
    "                bleu_scores.append(score)\n",
    "\n",
    "            \n",
    "            avg_bleu = np.mean(bleu_scores)\n",
    "            self.last_bleu = avg_bleu  # 마지막 BLEU 점수 저장\n",
    "            print(f\"Epoch {epoch}: Average BLEU = {avg_bleu:.4f}\")\n",
    "            \n",
    "            # BLEU 점수가 개선되었는지 확인합니다.\n",
    "            if avg_bleu > self.best_bleu:\n",
    "                self.best_bleu = avg_bleu\n",
    "                self.wait = 0\n",
    "            else:\n",
    "                self.wait += 1\n",
    "\n",
    "            # patience 또는 threshold 조건을 만족하면 조기 종료합니다.\n",
    "            if self.wait >= self.patience or avg_bleu >= self.threshold:\n",
    "                print(\"Early stopping triggered based on BLEU score.\")\n",
    "                self.model.stop_training = True\n",
    "\n",
    "\n",
    "    def get_last_bleu(self):\n",
    "        \"\"\"훈련 종료 후 마지막 BLEU 점수를 반환합니다.\"\"\"\n",
    "        return self.last_bleu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(questions)\n",
    "# 7439개를 훈련에 사용하므로 BLEU 평가에 1%를 사용 (너무 오래 걸려서)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 200\n",
    "# 평가용으로 사용할 원본 질문과 정답 텍스트\n",
    "sample_questions = questions[:75]\n",
    "sample_answers = answers[:75]\n",
    "\n",
    "bleu_early_stopping = BLEUEarlyStopping(\n",
    "    sample_questions=sample_questions,\n",
    "    sample_answers=sample_answers,\n",
    "    patience=5,\n",
    "    threshold=0.9,\n",
    "    check_interval= 10\n",
    ")\n",
    "\n",
    "# 훈련을 시작하고, 학습이 끝난 후 마지막 BLEU 점수를 출력합니다.\n",
    "history = model.fit(\n",
    "    dataset,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[bleu_early_stopping],  # CSVLogger와 BLEUEarlyStopping 콜백 추가\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 훈련 종료 후 마지막 BLEU 점수 확인\n",
    "last_bleu = bleu_early_stopping.get_last_bleu()\n",
    "print(f\"Final BLEU score: {last_bleu:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5. 모델 평가하기\n",
    "\n",
    "Step 1에서 선택한 전처리 방법을 고려하여 입력된 문장에 대해서 대답을 얻는 예측 함수를 만듭니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_inference(sentence):\n",
    "  sentence = preprocess_korean_sentence(sentence)\n",
    "\n",
    "  # 입력된 문장을 정수 인코딩 후, 시작 토큰과 종료 토큰을 앞뒤로 추가.\n",
    "  # ex) Where have you been? → [[8331   86   30    5 1059    7 8332]]\n",
    "  sentence = tf.expand_dims(\n",
    "      START_TOKEN + tokenizer.encode(sentence) + END_TOKEN, axis=0)\n",
    "\n",
    "  # 디코더의 현재까지의 예측한 출력 시퀀스가 지속적으로 저장되는 변수.\n",
    "  # 처음에는 예측한 내용이 없음으로 시작 토큰만 별도 저장. ex) 8331\n",
    "  output_sequence = tf.expand_dims(START_TOKEN, 0)\n",
    "\n",
    "  # 디코더의 인퍼런스 단계\n",
    "  for i in range(MAX_LENGTH):\n",
    "    # 디코더는 최대 MAX_LENGTH의 길이만큼 다음 단어 예측을 반복합니다.\n",
    "    predictions = model(inputs=[sentence, output_sequence], training=False)\n",
    "    predictions = predictions[:, -1:, :]\n",
    "\n",
    "    # 현재 예측한 단어의 정수\n",
    "    predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "\n",
    "    # 만약 현재 예측한 단어가 종료 토큰이라면 for문을 종료\n",
    "    if tf.equal(predicted_id, END_TOKEN[0]):\n",
    "      break\n",
    "\n",
    "    # 예측한 단어들은 지속적으로 output_sequence에 추가됩니다.\n",
    "    # 이 output_sequence는 다시 디코더의 입력이 됩니다.\n",
    "    output_sequence = tf.concat([output_sequence, predicted_id], axis=-1)\n",
    "\n",
    "  return tf.squeeze(output_sequence, axis=0)\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_generation(sentence):\n",
    "  # 입력 문장에 대해서 디코더를 동작 시켜 예측된 정수 시퀀스를 리턴받습니다.\n",
    "  prediction = decoder_inference(sentence)\n",
    "\n",
    "  # 정수 시퀀스를 다시 텍스트 시퀀스로 변환합니다.\n",
    "  predicted_sentence = tokenizer.decode(\n",
    "      [i for i in prediction if i < tokenizer.vocab_size])\n",
    "\n",
    "  print('입력 : {}'.format(sentence))\n",
    "  print('출력 : {}'.format(predicted_sentence))\n",
    "\n",
    "  return predicted_sentence\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 최종 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_generation('오늘 날씨 어때?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_generation('배가 너무 아파')"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ver0에서 아무런 제약 없이 100epoch를 돌렸을 때의 결과 입니다.\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLEU 점수에 1-gram만을 기준으로 30epoch를 돌렸을 때의 결과입니다.\n",
    "\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 회고\n",
    "0. ver0에서는 accuracy가 0.1583으로 학습이 마무리 되었는데 최종 결과를 확인하면 성능이 너무 좋게 나와서 sparse categorical accuracy가 어떻게 측정되는지 알아보았다.\n",
    "    - y_pred는 model의 최종 출력으로 (batch_size, seq_len, vocab_size)이다.\n",
    "    - sparse categorical loss에 y_true와 y_pred가 매개변수로 들어가고 from_logits=True이면, y_pred는 argmax가 적용된다고 한다.\n",
    "    - 그렇다면 accuracy는 예측 토큰 끼리의 정확도를 의미한다.\n",
    "    - 결론적으로 생성 모델에서 accuracy보다 정성평가가 더 중요하다는걸 정말 많이 느끼게 되었다.\n",
    "    - **다른 동료 분들과 코드를 공유하면서 확인해보니 학습에 사용할 문장의 최대 길이가 데이터셋이 나오는 문장들보다 너무 길어서 accuracy에 문제가 생겼던 걸 확인할 수 있었고 이를 추후에 수정할 계획이다**\n",
    "\n",
    "1. 최종에서는 문장 최대 길이를 수정하여 이전보다 납득할 수 있는 accuracy 0.4768에서 학습을 마무리하였다. 이전 시도에서 accuracy기반으로 평가할 수 없다는 것을 정리할 수 있어서, BLEU 점수를 도입하였고 문장 길이가 짧아 1-gram 기준으로만 평가하여 0.9이상이 나오면 학습을 종료하도록 설정하였다. 10번의 epoch마다 BLEU점수를 측정하였으며 30번째 epoch에서 0.9733 점수를 기록하게 되어 학습을 멈추게 되었다.\n",
    "    - accuracy가 0.4768인데에 반해, BLEU는 0.9733인 것으로 보아 padding으로 인해 정확한 accuracy 측정이 어려운 것으로 예상하고 추후에 더 자세히 찾아보고 정리해보아야겠다.\n",
    "    - BLEU가 0.9733이라면 대부분의 답변이 dataset에 있는 답변 그대로를 출력하는 모습일 것으로 예상이 된다. 적절한 Threshold 설정이 정말 중요하겠다는 것을 확실히 체감할 수 있었다.\n",
    "    - 신기한 점은 ver0에서는 \"배가 너무 아파\" 라는 질문에  \"좀 쉬세요\" 라는 그럴듯한 답변을 얻을 수 있었는데 최종 버전에서는 \"시원한 좀 쉬고 약먹기\" 라는 이상한 답변을 얻을 수 있었다.\n",
    "    - 이 결과는 1-gram만을 기준으로 평가했기 때문이란 것을 알 수 있었다. 문장 길이가 짧아 학습하는 과정에서 2-gram이상 겹치는 항목이 없어 별도의 조치가 필요하다는 로그가 출력이 되어 1-gram만을 기준으로 해도 괜찮을거라 생각했지만 확실히 1-gram만 평가했을 때에는 단어 간의 관계나 문맥이 반영되지 않기 때문에 이런 문제가 발생했다고 생각하고 1~4gram 모두 평가될 수 있도록 설정하여 다시 학습시켰다.\n",
    "    - \n",
    "\n",
    "2. 전체적인 프로젝트 요구사항보다 먼저 한국어 텍스트 전처리에 대해 고민할 때, vocab 크기가 커지더라도 빈도 수를 고려해서 토큰을 줄이지 말아야겠다는 결론을 내렸었다. 왜냐하면 드물게 등장하는 토큰 중 해당 문장에서 중요한 의미를 가지는 경우가 많다고 판단했기 때문이다. 하지만 프로젝트의 요구사항에 따라 전처리를 따라가다 보니 어떤 토큰을 명시적으로 삭제하지 않고 vocab 크기를 먼저 결정짓는 방식의 토큰화를 진행하게 되었다. 해당 토큰화 방식이 어떻게 이루어지는지 더 자세히 살펴보았다.\n",
    "    - 서브워드 토큰화 원리\n",
    "    - 예. birdstrike => bird와 strike의 서브워드로 분리\n",
    "    - 주어진 말뭉치(corpus; 훈련데이터셋)에서 가장 자주 등장하는 서브워드 단위를 찾아 vocab를 구축한다.\n",
    "    - Greedy 토큰화; 문장을 토큰화할 때, 가장 긴 매칭 서브워드부터 시작하여 점진적으로 작은 단위로 분할한다.\n",
    "    - UNKOWN 처리; 어휘에 없는 단어는 더 작은 서브워드 단위로 분해된다. 개별 문자 단위까지도 분해할 수 있다.\n",
    "    - 형태학적으로 복잡한 언어나 신조어가 많은 도메인에 유용하다\n",
    "    - 결국 vocab을 구성하게 되는 방식은 항상 토큰의 등장 빈도수에 따라 결정되는 것 처럼 느껴진다. 과연 항상 그럴까?\n",
    "\n",
    "3. 이 내용은 Transformer 구조를 세세하게 뜯어 볼때 생겼던 의문 사항과 결론으로 다음에 헷갈리지 않기 위해 기록해놓으려 한다.\n",
    "    - tensor나 matrix 등 축이 3개 이상으로 많아질 때, Transpose를 적용하면 맨 마지막 2 축에만 적용된다.\n",
    "    - LSTM이나 RNN에서 유사한 태스크에 대해 학습할 때, 생각해보면 padding mask 라는걸 사용하지 않는 것을 알 수 있다. 왜냐하면 입력 sequence에서 패딩은 0으로 입력하기 때문에 복잡한 네트워크를 지나더라도 0이 출력되어 계산에 반영이 안되기 때문이다. 하지만 Transformer에서는 padding mask를 사용하는데 그 이유는 똑같이 0으로 출력되지만 attention_weight(어텐션 점수)를 계산할 때에는 Softmax 함수를 사용하기 때문에, 'e^0'은 '1'이 되어 패딩이 계산에 관여하게 되는 문제가 생긴다. 따라서 어떤 위치에 패딩이 들어갔는지 기억하고 해당 위치에 매우 큰 음수를 곱해서 0에 가깝게 만들어 줘야 한다. 이러한 이유로 padding mask가 필요하다.\n",
    "    - [포지셔널 인코딩](https://medium.com/thedeephub/positional-encoding-explained-a-deep-dive-into-transformer-pe-65cfe8cfe10b)에서 뒤로 갈수록 값의 변화가 없고 앞쪽으로 올수록 값의 변화가 잦은 이유는 pos/var 가 주기 함수의 변수로 들어가는데 뒤로 갈수록 var의 크기가 커져 주기가 늘어나는 효과가 나타나기 때문이다. 그럼 왜 이렇게 구현했을까? 논문의 저자는 \"모델이 상대적 위치에 따라 주의를 기울이는 법을 쉽게 학습할 수 있을 것\"이라는 가설을 세웠으며 이에 따라 고정된 오프셋k에 대해 PE(pos+k)는 PE(pos)의 선형 함수로 표현될 수 있다라고 하였다. 이 말은 PE(pos)가 PE(pos+K)로 이동할 수 있게 만드는 행렬 M이 존재하고 이 행렬 M은 pos와 독립적이라는 의미라고 한다. 즉, 시작하는 절대 위치와 관계없이(pos가 변화하여도) 같은 방식으로(M*PE(pos)로) 작동한다는 뜻이다. 따라서 절대적인 위치 정보를 사용하지 않고도 상대적인 위치 정보를 학습할 수 있도록 한다는 의미이다. 예. 'not'이 문장에서 어디에 나타나는지에 관계 없이 근처 단어의 의미를 수정한다는 것을 학습할 수 있다는 의미.\n",
    "\n",
    "        - cf.[불용어 사전](https://gist.github.com/spikeekips/40eea22ef4a89f629abd87eed535ac6a) : 불용어 사전을 가져온 github 주소입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
