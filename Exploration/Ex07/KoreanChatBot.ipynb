{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10-1. 프로젝트: 한국어 데이터로 챗봇 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1 & 2 데이터 수집하기 & 데이터 전처리하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12시 땡!</td>\n",
       "      <td>하루가 또 가네요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1지망 학교 떨어졌어</td>\n",
       "      <td>위로해 드립니다.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3박4일 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3박4일 정도 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PPL 심하네</td>\n",
       "      <td>눈살이 찌푸려지죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Q            A  label\n",
       "0           12시 땡!   하루가 또 가네요.      0\n",
       "1      1지망 학교 떨어졌어    위로해 드립니다.      0\n",
       "2     3박4일 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
       "3  3박4일 정도 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
       "4          PPL 심하네   눈살이 찌푸려지죠.      0"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('ChatbotData.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11823"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "중복된 헤드라인: 결혼이나 하지 왜 자꾸 나한테 화 내냐구!\n",
      "중복된 행의 인덱스: [152, 5527]\n",
      "\n",
      "중복된 헤드라인: 고백하고 후회하면 어떡하지\n",
      "중복된 행의 인덱스: [189, 5537]\n",
      "\n",
      "중복된 헤드라인: 고양이 키우고 싶어\n",
      "중복된 행의 인덱스: [195, 196]\n",
      "\n",
      "중복된 헤드라인: 공부는 내 체질이 아닌 것 같아\n",
      "중복된 행의 인덱스: [226, 5542]\n",
      "\n",
      "중복된 헤드라인: 공시 준비 힘들어\n",
      "중복된 행의 인덱스: [234, 235]\n",
      "\n",
      "중복된 헤드라인: 기숙사 괜찮을까\n",
      "중복된 행의 인덱스: [377, 5704]\n",
      "\n",
      "중복된 헤드라인: 나는 좋은데 ….\n",
      "중복된 행의 인덱스: [592, 5774]\n",
      "\n",
      "중복된 헤드라인: 남동생한테 자꾸 화내게 되네\n",
      "중복된 행의 인덱스: [703, 5828]\n",
      "\n",
      "중복된 헤드라인: 남자인지 여자인지 알려줘\n",
      "중복된 행의 인덱스: [720, 5839]\n",
      "\n",
      "중복된 헤드라인: 낭만이라고는 없어가지구\n",
      "중복된 행의 인덱스: [782, 5848]\n",
      "\n",
      "중복된 헤드라인: 내 이름이 없어\n",
      "중복된 행의 인덱스: [808, 5865]\n",
      "\n",
      "중복된 헤드라인: 내가 뭘 좋아하는지 잘하는지 모르겠어\n",
      "중복된 행의 인덱스: [842, 5875]\n",
      "\n",
      "중복된 헤드라인: 내일 만나자고 해볼까?\n",
      "중복된 행의 인덱스: [890, 9541]\n",
      "\n",
      "중복된 헤드라인: 너 만든 사람은 누구야?\n",
      "중복된 행의 인덱스: [931, 5901]\n",
      "\n",
      "중복된 헤드라인: 너무 힘들다\n",
      "중복된 행의 인덱스: [980, 5944, 5945]\n",
      "\n",
      "중복된 헤드라인: 너무하네 진짜\n",
      "중복된 행의 인덱스: [982, 5963]\n",
      "\n",
      "중복된 헤드라인: 넘 많이 먹었다.\n",
      "중복된 행의 인덱스: [985, 5982]\n",
      "\n",
      "중복된 헤드라인: 놀아 줄 사람이 없어\n",
      "중복된 행의 인덱스: [1026, 5991]\n",
      "\n",
      "중복된 헤드라인: 눈썹 문신 어때?\n",
      "중복된 행의 인덱스: [1070, 6003]\n",
      "\n",
      "중복된 헤드라인: 뉴스는 역시 지루해\n",
      "중복된 행의 인덱스: [1088, 6004]\n",
      "\n",
      "중복된 헤드라인: 돈 벌고 싶어\n",
      "중복된 행의 인덱스: [1293, 1294]\n",
      "\n",
      "중복된 헤드라인: 딱 좋아\n",
      "중복된 행의 인덱스: [1386, 6141]\n",
      "\n",
      "중복된 헤드라인: 또 전화 안받아\n",
      "중복된 행의 인덱스: [1412, 6164]\n",
      "\n",
      "중복된 헤드라인: 로또 번호 알려줘\n",
      "중복된 행의 인덱스: [1444, 1445]\n",
      "\n",
      "중복된 헤드라인: 마음이 울적해\n",
      "중복된 행의 인덱스: [1480, 1481]\n",
      "\n",
      "중복된 헤드라인: 만나기만 하면 싸워\n",
      "중복된 행의 인덱스: [1500, 1501]\n",
      "\n",
      "중복된 헤드라인: 말이 안 통해\n",
      "중복된 행의 인덱스: [1536, 1537]\n",
      "\n",
      "중복된 헤드라인: 면도 귀찬하\n",
      "중복된 행의 인덱스: [1662, 6309]\n",
      "\n",
      "중복된 헤드라인: 면접 잘 볼 수 있을까\n",
      "중복된 행의 인덱스: [1676, 6310]\n",
      "\n",
      "중복된 헤드라인: 면접 준비 방법\n",
      "중복된 행의 인덱스: [1677, 6311]\n",
      "\n",
      "중복된 헤드라인: 문 잠겼는데 집에 아무도 없네\n",
      "중복된 행의 인덱스: [1773, 6363]\n",
      "\n",
      "중복된 헤드라인: 뭐하면 시간이 잘 갈까\n",
      "중복된 행의 인덱스: [1834, 1835]\n",
      "\n",
      "중복된 헤드라인: 뭔가 무섭다\n",
      "중복된 행의 인덱스: [1845, 6378]\n",
      "\n",
      "중복된 헤드라인: 바라는게 없어\n",
      "중복된 행의 인덱스: [1897, 1898]\n",
      "\n",
      "중복된 헤드라인: 반가워\n",
      "중복된 행의 인덱스: [1913, 1914]\n",
      "\n",
      "중복된 헤드라인: 발목 접질렀어\n",
      "중복된 행의 인덱스: [1945, 1946]\n",
      "\n",
      "중복된 헤드라인: 배고파\n",
      "중복된 행의 인덱스: [2000, 2001]\n",
      "\n",
      "중복된 헤드라인: 배불러\n",
      "중복된 행의 인덱스: [2013, 2014]\n",
      "\n",
      "중복된 헤드라인: 비 오는데?\n",
      "중복된 행의 인덱스: [2197, 2198]\n",
      "\n",
      "중복된 헤드라인: 비밀번호 뭐였더라\n",
      "중복된 행의 인덱스: [2208, 6552]\n",
      "\n",
      "중복된 헤드라인: 뻔하다\n",
      "중복된 행의 인덱스: [2251, 6561]\n",
      "\n",
      "중복된 헤드라인: 사랑한다고 말해줘\n",
      "중복된 행의 인덱스: [2283, 10001]\n",
      "\n",
      "중복된 헤드라인: 사업 시작해도 될까\n",
      "중복된 행의 인덱스: [2291, 6611]\n",
      "\n",
      "중복된 헤드라인: 생리통 때문에 배 아파\n",
      "중복된 행의 인덱스: [2414, 6677]\n",
      "\n",
      "중복된 헤드라인: 선풍기 틀어도 더워\n",
      "중복된 행의 인덱스: [2456, 2457]\n",
      "\n",
      "중복된 헤드라인: 셀프웨딩이 유행이래\n",
      "중복된 행의 인덱스: [2504, 6700]\n",
      "\n",
      "중복된 헤드라인: 속이 안 좋아\n",
      "중복된 행의 인덱스: [2535, 2536]\n",
      "\n",
      "중복된 헤드라인: 수업시간 내내 잤어\n",
      "중복된 행의 인덱스: [2584, 2585]\n",
      "\n",
      "중복된 헤드라인: 술 마시고 싶다\n",
      "중복된 행의 인덱스: [2616, 2617]\n",
      "\n",
      "중복된 헤드라인: 술 먹고 싶어\n",
      "중복된 행의 인덱스: [2620, 6718]\n",
      "\n",
      "중복된 헤드라인: 술만 먹으면 전화를 안 받아\n",
      "중복된 행의 인덱스: [2636, 6727]\n",
      "\n",
      "중복된 헤드라인: 쉬는 중입니다.\n",
      "중복된 행의 인덱스: [2644, 6747]\n",
      "\n",
      "중복된 헤드라인: 스타트업하면 위험할까\n",
      "중복된 행의 인덱스: [2675, 6751]\n",
      "\n",
      "중복된 헤드라인: 스트레스 받아\n",
      "중복된 행의 인덱스: [2685, 2686]\n",
      "\n",
      "중복된 헤드라인: 신혼여행 가서 돌아오기 싫다\n",
      "중복된 행의 인덱스: [2777, 6787]\n",
      "\n",
      "중복된 헤드라인: 심심해\n",
      "중복된 행의 인덱스: [2801, 6796]\n",
      "\n",
      "중복된 헤드라인: 썸타는거 친구한테 이야기 한고 싶다.\n",
      "중복된 행의 인덱스: [2820, 6802]\n",
      "\n",
      "중복된 헤드라인: 아무도 안 놀아줘\n",
      "중복된 행의 인덱스: [2859, 6839]\n",
      "\n",
      "중복된 헤드라인: 아이디 생각 안나\n",
      "중복된 행의 인덱스: [2893, 6849]\n",
      "\n",
      "중복된 헤드라인: 아파\n",
      "중복된 행의 인덱스: [2922, 6882]\n",
      "\n",
      "중복된 헤드라인: 애를 나혼자 키우는 것 같아\n",
      "중복된 행의 인덱스: [2988, 6907]\n",
      "\n",
      "중복된 헤드라인: 야\n",
      "중복된 행의 인덱스: [2995, 6909]\n",
      "\n",
      "중복된 헤드라인: 얘는 싸우기만 하면 연락이 안돼\n",
      "중복된 행의 인덱스: [3022, 6918]\n",
      "\n",
      "중복된 헤드라인: 언제쯤 예쁘게 화장 잘할까\n",
      "중복된 행의 인덱스: [3065, 7013]\n",
      "\n",
      "중복된 헤드라인: 엄마아빠랑 다시 같이 살아야돼\n",
      "중복된 행의 인덱스: [3111, 7030]\n",
      "\n",
      "중복된 헤드라인: 업무 스트레스 넘 심해\n",
      "중복된 행의 인덱스: [3121, 7031]\n",
      "\n",
      "중복된 헤드라인: 오싹한 이야기 해줄까\n",
      "중복된 행의 인덱스: [3375, 7210]\n",
      "\n",
      "중복된 헤드라인: 올해 왜 이러지\n",
      "중복된 행의 인덱스: [3387, 3388, 3389]\n",
      "\n",
      "중복된 헤드라인: 외로워\n",
      "중복된 행의 인덱스: [3456, 3457, 7261, 7262]\n",
      "\n",
      "중복된 헤드라인: 우정이란게 뭘까\n",
      "중복된 행의 인덱스: [3532, 7292]\n",
      "\n",
      "중복된 헤드라인: 웃겨봐\n",
      "중복된 행의 인덱스: [3564, 7299]\n",
      "\n",
      "중복된 헤드라인: 이 순간 뭘하면 좋을까\n",
      "중복된 행의 인덱스: [3654, 7343]\n",
      "\n",
      "중복된 헤드라인: 일이 안 끝나\n",
      "중복된 행의 인덱스: [3834, 7786]\n",
      "\n",
      "중복된 헤드라인: 일이 익숙해 지지 않네\n",
      "중복된 행의 인덱스: [3835, 7787]\n",
      "\n",
      "중복된 헤드라인: 임용 기다리고 있어\n",
      "중복된 행의 인덱스: [3858, 3859]\n",
      "\n",
      "중복된 헤드라인: 자꾸 봐주니까 기어오른다\n",
      "중복된 행의 인덱스: [3909, 7843]\n",
      "\n",
      "중복된 헤드라인: 자꾸 졸게 되네\n",
      "중복된 행의 인덱스: [3916, 7849]\n",
      "\n",
      "중복된 헤드라인: 자니?\n",
      "중복된 행의 인덱스: [3921, 3922]\n",
      "\n",
      "중복된 헤드라인: 자존감이 너무 떨어졌어\n",
      "중복된 행의 인덱스: [3945, 3946]\n",
      "\n",
      "중복된 헤드라인: 저 사람이 왜 자꾸 볼까?\n",
      "중복된 행의 인덱스: [4028, 7941]\n",
      "\n",
      "중복된 헤드라인: 주차장이 꽉 찼어\n",
      "중복된 행의 인덱스: [4235, 8159]\n",
      "\n",
      "중복된 헤드라인: 중2 히스테리 알아?\n",
      "중복된 행의 인덱스: [4249, 8176]\n",
      "\n",
      "중복된 헤드라인: 중국어 혼자 공부 가능한가?\n",
      "중복된 행의 인덱스: [4264, 8177]\n",
      "\n",
      "중복된 헤드라인: 최저임금 수준에서 알바비가 안 나왔어\n",
      "중복된 행의 인덱스: [4505, 8328]\n",
      "\n",
      "중복된 헤드라인: 출근시간 아깝다\n",
      "중복된 행의 인덱스: [4533, 8338]\n",
      "\n",
      "중복된 헤드라인: 카톡할 친구가 없네\n",
      "중복된 행의 인덱스: [4704, 4705]\n",
      "\n",
      "중복된 헤드라인: 컴터가 맛이 갔어\n",
      "중복된 행의 인덱스: [4744, 8359]\n",
      "\n",
      "중복된 헤드라인: 컴퓨터가 느려졌어\n",
      "중복된 행의 인덱스: [4748, 4749]\n",
      "\n",
      "중복된 헤드라인: 택시비 너무 비싸\n",
      "중복된 행의 인덱스: [4810, 8364]\n",
      "\n",
      "중복된 헤드라인: 평생 함께하고 싶다.\n",
      "중복된 행의 인덱스: [4880, 8381]\n",
      "\n",
      "중복된 헤드라인: 하고 싶은 게 없어\n",
      "중복된 행의 인덱스: [4946, 4947]\n",
      "\n",
      "중복된 헤드라인: 하고 싶은게 없어\n",
      "중복된 행의 인덱스: [4949, 8397]\n",
      "\n",
      "중복된 헤드라인: 할 줄 아는거!\n",
      "중복된 행의 인덱스: [5030, 8470]\n",
      "\n",
      "중복된 헤드라인: 핸드폰 꺼지기 직전\n",
      "중복된 행의 인덱스: [5058, 5059]\n",
      "\n",
      "중복된 헤드라인: 핸드폰 떨어뜨려서 고장 났나봐\n",
      "중복된 행의 인덱스: [5063, 8476]\n",
      "\n",
      "중복된 헤드라인: 혼자 살아야 할 듯\n",
      "중복된 행의 인덱스: [5146, 8741]\n",
      "\n",
      "중복된 헤드라인: 혼자 있으니까 편하네\n",
      "중복된 행의 인덱스: [5154, 8748]\n",
      "\n",
      "중복된 헤드라인: 혼자 잘 살 수 있을까?\n",
      "중복된 행의 인덱스: [5157, 8749]\n",
      "\n",
      "중복된 헤드라인: 혼자 해야 돼\n",
      "중복된 행의 인덱스: [5159, 8751]\n",
      "\n",
      "중복된 헤드라인: 화장실!!\n",
      "중복된 행의 인덱스: [5191, 8761]\n",
      "\n",
      "중복된 헤드라인: 환승 가능?\n",
      "중복된 행의 인덱스: [5207, 8764]\n",
      "\n",
      "중복된 헤드라인: 회사 사람들이 아직도 불편해\n",
      "중복된 행의 인덱스: [5218, 8780]\n",
      "\n",
      "중복된 헤드라인: 회사에는 왜 친구 같은 사람이 없을까\n",
      "중복된 행의 인덱스: [5232, 8782]\n",
      "\n",
      "중복된 헤드라인: 후련하달까\n",
      "중복된 행의 인덱스: [5246, 8789]\n",
      "\n",
      "중복된 헤드라인: 2년 가량의 연애\n",
      "중복된 행의 인덱스: [5316, 8868]\n",
      "\n",
      "중복된 헤드라인: 간단하게 사랑이라는건.\n",
      "중복된 행의 인덱스: [5438, 5439]\n",
      "\n",
      "중복된 헤드라인: 결국 핸드폰 번호 바꿨어\n",
      "중복된 행의 인덱스: [5508, 5509]\n",
      "\n",
      "중복된 헤드라인: 결국 헤어졌네\n",
      "중복된 행의 인덱스: [5510, 5511]\n",
      "\n",
      "중복된 헤드라인: 나도 모르게 니 생각을 하고 있어\n",
      "중복된 행의 인덱스: [5776, 9171]\n",
      "\n",
      "중복된 헤드라인: 너무 힘듭니다\n",
      "중복된 행의 인덱스: [5953, 5954]\n",
      "\n",
      "중복된 헤드라인: 미치겠네\n",
      "중복된 행의 인덱스: [6398, 6399]\n",
      "\n",
      "중복된 헤드라인: 미치겠습니다\n",
      "중복된 행의 인덱스: [6400, 6401]\n",
      "\n",
      "중복된 헤드라인: 사랑이 뭘까?\n",
      "중복된 행의 인덱스: [6600, 9923]\n",
      "\n",
      "중복된 헤드라인: 생각할 시간을 달라고 한 남친\n",
      "중복된 행의 인덱스: [6674, 6675]\n",
      "\n",
      "중복된 헤드라인: 연락하고 싶어\n",
      "중복된 행의 인덱스: [7095, 10709]\n",
      "\n",
      "중복된 헤드라인: 이별 6일째\n",
      "중복된 행의 인덱스: [7413, 7414]\n",
      "\n",
      "중복된 헤드라인: 이별이란\n",
      "중복된 행의 인덱스: [7561, 7562]\n",
      "\n",
      "중복된 헤드라인: 정말 힘드네\n",
      "중복된 행의 인덱스: [8067, 8068]\n",
      "\n",
      "중복된 헤드라인: 헤어진지 한달\n",
      "중복된 행의 인덱스: [8690, 8710]\n",
      "\n",
      "중복된 헤드라인: 가을 타나 봐.\n",
      "중복된 행의 인덱스: [8901, 8902]\n",
      "\n",
      "중복된 헤드라인: 과함 설렘 후에 지금은 안 설레\n",
      "중복된 행의 인덱스: [9017, 9018]\n",
      "\n",
      "중복된 헤드라인: 남자친구랑 말이 안 통해\n",
      "중복된 행의 인덱스: [9372, 9373]\n",
      "\n",
      "중복된 헤드라인: 남편이 자꾸 만져\n",
      "중복된 행의 인덱스: [9441, 9442]\n",
      "\n",
      "중복된 헤드라인: 내가 좋아할 자격이 있나 모르겠어.\n",
      "중복된 행의 인덱스: [9526, 9527]\n",
      "\n",
      "중복된 헤드라인: 내가 좋아해도 될까?\n",
      "중복된 행의 인덱스: [9528, 9529]\n",
      "\n",
      "중복된 헤드라인: 너무 행복해. 오래갈 수 있을까?\n",
      "중복된 행의 인덱스: [9559, 9560]\n",
      "\n",
      "중복된 헤드라인: 마음에 없는데 관계를 하고 싶대\n",
      "중복된 행의 인덱스: [9674, 9675]\n",
      "\n",
      "중복된 헤드라인: 봄 타나 봄.\n",
      "중복된 행의 인덱스: [9801, 9802]\n",
      "\n",
      "중복된 헤드라인: 불타올랐다가 식었어\n",
      "중복된 행의 인덱스: [9812, 9813]\n",
      "\n",
      "중복된 헤드라인: 사귀자는 말 아니면 썸인가\n",
      "중복된 행의 인덱스: [9833, 9834]\n",
      "\n",
      "중복된 헤드라인: 사랑에 빠진 거 같아\n",
      "중복된 행의 인덱스: [9881, 9882]\n",
      "\n",
      "중복된 헤드라인: 사랑을 했다\n",
      "중복된 행의 인덱스: [9897, 9898, 9899]\n",
      "\n",
      "중복된 헤드라인: 사랑이 뭐야?\n",
      "중복된 행의 인덱스: [9917, 9918]\n",
      "\n",
      "중복된 헤드라인: 사랑하는 사람이랑 결혼하고싶어\n",
      "중복된 행의 인덱스: [9962, 9963]\n",
      "\n",
      "중복된 헤드라인: 사랑하면 예뻐지나봐\n",
      "중복된 행의 인덱스: [9991, 9992]\n",
      "\n",
      "중복된 헤드라인: 사랑해\n",
      "중복된 행의 인덱스: [10014, 10015]\n",
      "\n",
      "중복된 헤드라인: 썸 타는 기간\n",
      "중복된 행의 인덱스: [10183, 10184]\n",
      "\n",
      "중복된 헤드라인: 썸이었으면 좋겠어\n",
      "중복된 행의 인덱스: [10380, 10381]\n",
      "\n",
      "중복된 헤드라인: 씨씨인데 몰래 만나고 있는데 공개 연애 하자고 할까?\n",
      "중복된 행의 인덱스: [10432, 10433]\n",
      "\n",
      "중복된 헤드라인: 아기 좋아하는 남자 어때?\n",
      "중복된 행의 인덱스: [10436, 10437]\n",
      "\n",
      "중복된 헤드라인: 아기 좋아하는 여자 어때?\n",
      "중복된 행의 인덱스: [10438, 10439]\n",
      "\n",
      "중복된 헤드라인: 여자친구가 연락이 안돼\n",
      "중복된 행의 인덱스: [10612, 10613]\n",
      "\n",
      "중복된 헤드라인: 여자친구한테 더 이상 설레지 않아\n",
      "중복된 행의 인덱스: [10670, 10671]\n",
      "\n",
      "중복된 헤드라인: 영화 보자는데 나한테 관심 있나?\n",
      "중복된 행의 인덱스: [10764, 10765]\n",
      "\n",
      "중복된 헤드라인: 이 사람 만나면서 내 자존감이 떨어지는 것 같아 안 만나는게 답이야?\n",
      "중복된 행의 인덱스: [10888, 10889]\n",
      "\n",
      "중복된 헤드라인: 인연이 있다고 생각해?\n",
      "중복된 행의 인덱스: [10939, 10940]\n",
      "\n",
      "중복된 헤드라인: 장난인지 진심인지 구분이 안돼.\n",
      "중복된 행의 인덱스: [10981, 10982]\n",
      "\n",
      "중복된 헤드라인: 좋아하는 감정이 나를 슬프게도 해\n",
      "중복된 행의 인덱스: [11029, 11030]\n",
      "\n",
      "중복된 헤드라인: 좋아하는 거랑 사랑하는 건 어떻게 구분해?\n",
      "중복된 행의 인덱스: [11034, 11035]\n",
      "\n",
      "중복된 헤드라인: 좋아하는 거랑 사랑하는게 다른 거야?\n",
      "중복된 행의 인덱스: [11036, 11037]\n",
      "\n",
      "중복된 헤드라인: 좋아하는지 확신이 안들어\n",
      "중복된 행의 인덱스: [11264, 11265]\n",
      "\n",
      "중복된 헤드라인: 착해서 잘해주는 건지 좋아하는 건지\n",
      "중복된 행의 인덱스: [11641, 11642]\n",
      "\n",
      "중복된 헤드라인: 첫 눈에 반하는게 가능해?\n",
      "중복된 행의 인덱스: [11648, 11649]\n",
      "\n",
      "중복된 헤드라인: 첫사랑 생각나\n",
      "중복된 행의 인덱스: [11657, 11658]\n",
      "\n",
      "중복된 헤드라인: 커플여행이 나을까 그냥 우리끼리 갈까?\n",
      "중복된 행의 인덱스: [11731, 11732]\n",
      "\n",
      "중복된 헤드라인: 훔쳐보는 것도 눈치 보임.\n",
      "중복된 행의 인덱스: [11818, 11819]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 중복인 열의 데이터를 확인하기\n",
    "duplicate_indices = []\n",
    "\n",
    "unique_Q = data['Q'].unique()\n",
    "\n",
    "for unique in unique_Q:\n",
    "    indices = data[data['Q'] == unique].index.tolist()\n",
    "    if len(indices) > 1:\n",
    "        print(f\"중복된 헤드라인: {unique}\")\n",
    "        print(f\"중복된 행의 인덱스: {indices}\")\n",
    "        duplicate_indices.extend(indices)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>결혼이나 하지 왜 자꾸 나한테 화 내냐구!</td>\n",
       "      <td>힘들겠네요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5527</th>\n",
       "      <td>결혼이나 하지 왜 자꾸 나한테 화 내냐구!</td>\n",
       "      <td>힘들겠네요.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>고백하고 후회하면 어떡하지</td>\n",
       "      <td>후회는 후회를 낳을뿐이에요. 용기 내세요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5537</th>\n",
       "      <td>고백하고 후회하면 어떡하지</td>\n",
       "      <td>후회는 후회를 낳을뿐이에요. 용기 내세요.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>고양이 키우고 싶어</td>\n",
       "      <td>자신을 먼저 키우세요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11658</th>\n",
       "      <td>첫사랑 생각나</td>\n",
       "      <td>지금의 사랑에 충실하세요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11731</th>\n",
       "      <td>커플여행이 나을까 그냥 우리끼리 갈까?</td>\n",
       "      <td>저는 둘이 가는 걸 좋아해요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11732</th>\n",
       "      <td>커플여행이 나을까 그냥 우리끼리 갈까?</td>\n",
       "      <td>저는 둘이 가는 게 좋아요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11818</th>\n",
       "      <td>훔쳐보는 것도 눈치 보임.</td>\n",
       "      <td>티가 나니까 눈치가 보이는 거죠!</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11819</th>\n",
       "      <td>훔쳐보는 것도 눈치 보임.</td>\n",
       "      <td>훔쳐보는 거 티나나봐요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>317 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Q                        A  label\n",
       "152    결혼이나 하지 왜 자꾸 나한테 화 내냐구!                   힘들겠네요.      0\n",
       "5527   결혼이나 하지 왜 자꾸 나한테 화 내냐구!                   힘들겠네요.      1\n",
       "189             고백하고 후회하면 어떡하지  후회는 후회를 낳을뿐이에요. 용기 내세요.      0\n",
       "5537            고백하고 후회하면 어떡하지  후회는 후회를 낳을뿐이에요. 용기 내세요.      1\n",
       "195                 고양이 키우고 싶어             자신을 먼저 키우세요.      0\n",
       "...                        ...                      ...    ...\n",
       "11658                  첫사랑 생각나           지금의 사랑에 충실하세요.      2\n",
       "11731    커플여행이 나을까 그냥 우리끼리 갈까?         저는 둘이 가는 걸 좋아해요.      2\n",
       "11732    커플여행이 나을까 그냥 우리끼리 갈까?          저는 둘이 가는 게 좋아요.      2\n",
       "11818           훔쳐보는 것도 눈치 보임.       티가 나니까 눈치가 보이는 거죠!      2\n",
       "11819           훔쳐보는 것도 눈치 보임.            훔쳐보는 거 티나나봐요.      2\n",
       "\n",
       "[317 rows x 3 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicate_data = data.loc[duplicate_indices]\n",
    "duplicate_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플수 : 7779\n"
     ]
    }
   ],
   "source": [
    "# 질문이 겹치는 경우, 항상 답변이 일치하지는 않는 것을 확인\n",
    "# => 답변을 기준으로 drop (같은 질문도 다르게 답변할 수 있도록)\n",
    "data.drop_duplicates(subset = ['A'], inplace=True)\n",
    "print('전체 샘플수 :', (len(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q        0\n",
      "A        0\n",
      "label    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(data.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "한국어와 영어 전처리 과정의 차이(https://haru0229.tistory.com/57)\n",
    "\n",
    "한국어는 영어와 달리 띄어쓰기로 구분하기 어렵다 (의존 형태소 떄문) => 띄어쓰기로 토큰화하는게 아니라 형태소 토큰화를 수행해야함 \n",
    "+ 전처리하면서 생긴 문제 및 해결방안\n",
    "1. 형태소 토큰화 수행\n",
    "2. 의존 형태소를 불용어 처리 (Q(질문)에만 적용 , A(답변)은 자연스럽게 출력될 수 있도록 불용어 처리X)\n",
    "3. 정규화 사전은 찾지 못함 (정규화하지 않으면 vocab 크기가 커지는 문제 뿐만 아니라 의미 있는 단어가 자주 등장하지 않는 토큰으로 처리되어 삭제될 수 있음 => 지금은 정규화 없이 진행 후 추후에 다시 고민)\n",
    "4. 영어가 나오면 우선 한글로 바꿈\n",
    "5. 숫자의 경우 한글로 바꾸지는 않음\n",
    "6. 3~5에서 발생하는 문제의 핵심은 결국 나중에 학습을 위해 드문 토큰들 삭제할 때, 지금 고민했던 토큰들이 삭제가 되는 것이 문제임, 우선 vocab크기가 너무 크지 않다면 따로 삭제하지 않는 것으로 진행 => 뒤에 전처리 요구사항에 SubwordTextEncoder를 사용하기 때문에 결국 어느정도 삭제될 수 밖에 없었음 (vocab size를 더 키우면 더 많이 보존할 수 있음; 회고에 추가 내용이 있습니다) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "에스에스디 제품은 12개의 기능이 있습니다 가격은 1500원\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def english_to_korean(word):\n",
    "    \"\"\"\n",
    "    영어 단어를 한글 발음으로 변환\n",
    "    예) SSD -> 에스에스디\n",
    "    \"\"\"\n",
    "    mapping = {\n",
    "        'a': '에이', 'b': '비', 'c': '씨', 'd': '디', 'e': '이',\n",
    "        'f': '에프', 'g': '지', 'h': '에이치', 'i': '아이',\n",
    "        'j': '제이', 'k': '케이', 'l': '엘', 'm': '엠',\n",
    "        'n': '엔', 'o': '오', 'p': '피', 'q': '큐',\n",
    "        'r': '알', 's': '에스', 't': '티', 'u': '유', \n",
    "        'v': '브이', 'w': '더블유', 'x': '엑스', 'y': '와이', 'z': '제트'\n",
    "    }\n",
    "    return \"\".join(mapping.get(ch.lower(), ch) for ch in word)\n",
    "\n",
    "def preprocess_korean_sentence(sentence, remove_stopwords=True):\n",
    "    \"\"\"\n",
    "    1. 앞뒤 공백 제거\n",
    "    2. 영어 단어는 한글 발음으로 변환 (예: SSD -> 에스에스디)\n",
    "    3. 최종적으로 한글 숫자 이외의 문자들을 제거\n",
    "    \"\"\"\n",
    "    # 1. 앞뒤 공백 제거\n",
    "    sentence = sentence.strip()\n",
    "       \n",
    "    # 2. 영어 변환: [A-Za-z]+ 패턴에 대해 영어 단어를 한글 발음으로 치환\n",
    "    sentence = re.sub(r'[A-Za-z]+', lambda m: english_to_korean(m.group()), sentence)\n",
    "    \n",
    "    # 3. 한글, 숫자, 공백을 제외한 문자 제거\n",
    "    sentence = re.sub(r'[^가-힣0-9\\s]', '', sentence)\n",
    "    \n",
    "    \n",
    "    return sentence\n",
    "\n",
    "# 예시 사용\n",
    "example_text = \"SSD 제품은 12개의 기능이 있습니다. 가격은 1500원.\"\n",
    "print(preprocess_korean_sentence(example_text, remove_stopwords=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'피피엘 심하네'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.reset_index(inplace=True, drop=True)\n",
    "\n",
    "preprocess_korean_sentence(data.loc[3,'Q'])\n",
    "#영어가 한국어로 잘 변환되는 것을 확인, 현재 데이터셋에서 대부분 영어는 약어로 쓰이기 때문에\n",
    "#이렇게 전처리하는게 적절하다고 판단"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "      <th>label</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12시 땡!</td>\n",
       "      <td>하루가 또 가네요.</td>\n",
       "      <td>0</td>\n",
       "      <td>12시 땡</td>\n",
       "      <td>하루가 또 가네요</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1지망 학교 떨어졌어</td>\n",
       "      <td>위로해 드립니다.</td>\n",
       "      <td>0</td>\n",
       "      <td>1지망 학교 떨어졌어</td>\n",
       "      <td>위로해 드립니다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3박4일 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "      <td>3박4일 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PPL 심하네</td>\n",
       "      <td>눈살이 찌푸려지죠.</td>\n",
       "      <td>0</td>\n",
       "      <td>피피엘 심하네</td>\n",
       "      <td>눈살이 찌푸려지죠</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SD카드 망가졌어</td>\n",
       "      <td>다시 새로 사는 게 마음 편해요.</td>\n",
       "      <td>0</td>\n",
       "      <td>에스디카드 망가졌어</td>\n",
       "      <td>다시 새로 사는 게 마음 편해요</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Q                   A  label      question             answer\n",
       "0        12시 땡!          하루가 또 가네요.      0         12시 땡          하루가 또 가네요\n",
       "1   1지망 학교 떨어졌어           위로해 드립니다.      0   1지망 학교 떨어졌어           위로해 드립니다\n",
       "2  3박4일 놀러가고 싶다         여행은 언제나 좋죠.      0  3박4일 놀러가고 싶다         여행은 언제나 좋죠\n",
       "3       PPL 심하네          눈살이 찌푸려지죠.      0       피피엘 심하네          눈살이 찌푸려지죠\n",
       "4     SD카드 망가졌어  다시 새로 사는 게 마음 편해요.      0    에스디카드 망가졌어  다시 새로 사는 게 마음 편해요"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(len(data)):\n",
    "   data.loc[i,'question'] = preprocess_korean_sentence(data.loc[i,'Q'])\n",
    "   data.loc[i,'answer'] = preprocess_korean_sentence(data.loc[i,'A'])\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3. SubwordTextEncoder 사용하기\n",
    "한국어 데이터는 형태소 분석기를 사용하여 토크나이징을 해야 한다고 많은 분이 알고 있습니다. 하지만 여기서는 형태소 분석기가 아닌 위 실습에서 사용했던 내부 단어 토크나이저인 SubwordTextEncoder를 그대로 사용해보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = data['question']\n",
    "answers = data['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "살짝 오래 걸릴 수 있어요. 스트레칭 한 번 해볼까요? 👐\n",
      "슝=3 \n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "print(\"살짝 오래 걸릴 수 있어요. 스트레칭 한 번 해볼까요? 👐\")\n",
    "\n",
    "# 질문과 답변 데이터셋에 대해서 Vocabulary 생성\n",
    "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(questions + answers, target_vocab_size=2**13)\n",
    "print(\"슝=3 \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# 시작 토큰과 종료 토큰에 고유한 정수를 부여합니다.\n",
    "START_TOKEN, END_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size + 1]\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START_TOKEN의 번호 : [8108]\n",
      "END_TOKEN의 번호 : [8109]\n"
     ]
    }
   ],
   "source": [
    "print('START_TOKEN의 번호 :' ,[tokenizer.vocab_size])\n",
    "print('END_TOKEN의 번호 :' ,[tokenizer.vocab_size + 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8110\n"
     ]
    }
   ],
   "source": [
    "# 시작 토큰과 종료 토큰을 고려하여 +2를 하여 단어장의 크기를 산정합니다.\n",
    "VOCAB_SIZE = tokenizer.vocab_size + 2\n",
    "print(VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정수 인코딩 후의 21번째 질문 샘플: [5240, 1087, 134, 524, 60]\n",
      "정수 인코딩 후의 21번째 답변 샘플: [1782, 6894, 6044, 7884, 61, 2789, 750]\n"
     ]
    }
   ],
   "source": [
    "# 임의의 22번째 샘플에 대해서 정수 인코딩 작업을 수행.\n",
    "# 각 토큰을 고유한 정수로 변환\n",
    "print('정수 인코딩 후의 21번째 질문 샘플: {}'.format(tokenizer.encode(questions[21])))\n",
    "print('정수 인코딩 후의 21번째 답변 샘플: {}'.format(tokenizer.encode(answers[21])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# 샘플의 최대 허용 길이 또는 패딩 후의 최종 길이\n",
    "MAX_LENGTH = 40\n",
    "\n",
    "# 정수 인코딩, 최대 길이를 초과하는 샘플 제거, 패딩\n",
    "def tokenize_and_filter(inputs, outputs):\n",
    "  tokenized_inputs, tokenized_outputs = [], []\n",
    "  \n",
    "  for (sentence1, sentence2) in zip(inputs, outputs):\n",
    "    # 정수 인코딩 과정에서 시작 토큰과 종료 토큰을 추가\n",
    "    sentence1 = START_TOKEN + tokenizer.encode(sentence1) + END_TOKEN\n",
    "    sentence2 = START_TOKEN + tokenizer.encode(sentence2) + END_TOKEN\n",
    "\n",
    "    # 최대 길이 40 이하인 경우에만 데이터셋으로 허용\n",
    "    if len(sentence1) <= MAX_LENGTH and len(sentence2) <= MAX_LENGTH:\n",
    "      tokenized_inputs.append(sentence1)\n",
    "      tokenized_outputs.append(sentence2)\n",
    "  \n",
    "  # 최대 길이 40으로 모든 데이터셋을 패딩\n",
    "  tokenized_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "      tokenized_inputs, maxlen=MAX_LENGTH, padding='post')\n",
    "  tokenized_outputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "      tokenized_outputs, maxlen=MAX_LENGTH, padding='post')\n",
    "  \n",
    "  return tokenized_inputs, tokenized_outputs\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어장의 크기 : 8110\n",
      "필터링 후의 질문 샘플 개수: 7779\n",
      "필터링 후의 답변 샘플 개수: 7779\n"
     ]
    }
   ],
   "source": [
    "questions, answers = tokenize_and_filter(questions, answers)\n",
    "print('단어장의 크기 :',(VOCAB_SIZE))\n",
    "print('필터링 후의 질문 샘플 개수: {}'.format(len(questions)))\n",
    "print('필터링 후의 답변 샘플 개수: {}'.format(len(answers)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-14 13:23:29.079537: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:08:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-02-14 13:23:29.101952: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:08:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-02-14 13:23:29.102008: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:08:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-02-14 13:23:29.105932: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:08:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-02-14 13:23:29.105972: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:08:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-02-14 13:23:29.106001: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:08:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-02-14 13:23:29.246691: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:08:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-02-14 13:23:29.246760: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:08:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-02-14 13:23:29.246767: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1726] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2025-02-14 13:23:29.246803: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:08:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-02-14 13:23:29.246829: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9711 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060, pci bus id: 0000:08:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 20000\n",
    "\n",
    "# 디코더는 이전의 target을 다음의 input으로 사용합니다.\n",
    "# 이에 따라 outputs에서는 START_TOKEN을 제거하겠습니다.\n",
    "dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {\n",
    "        'inputs': questions,\n",
    "        'dec_inputs': answers[:, :-1]\n",
    "    },\n",
    "    {\n",
    "        'outputs': answers[:, 1:]\n",
    "    },\n",
    "))\n",
    "\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4. 모델 구성하기\n",
    "위 실습 내용을 참고하여 트랜스포머 모델을 구현합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# 포지셔널 인코딩 레이어\n",
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "\n",
    "  def __init__(self, position, d_model):\n",
    "    super(PositionalEncoding, self).__init__()\n",
    "    self.pos_encoding = self.positional_encoding(position, d_model)\n",
    "\n",
    "  def get_angles(self, position, i, d_model):\n",
    "    angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
    "    return position * angles\n",
    "\n",
    "  def positional_encoding(self, position, d_model):\n",
    "    # 각도 배열 생성\n",
    "    angle_rads = self.get_angles(\n",
    "        position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n",
    "        i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],\n",
    "        d_model=d_model)\n",
    "\n",
    "    # 배열의 짝수 인덱스에는 sin 함수 적용\n",
    "    sines = tf.math.sin(angle_rads[:, 0::2])\n",
    "    # 배열의 홀수 인덱스에는 cosine 함수 적용\n",
    "    cosines = tf.math.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    # sin과 cosine이 교차되도록 재배열\n",
    "    pos_encoding = tf.stack([sines, cosines], axis=0)\n",
    "    pos_encoding = tf.transpose(pos_encoding,[1, 2, 0]) \n",
    "    pos_encoding = tf.reshape(pos_encoding, [position, d_model])\n",
    "\n",
    "    pos_encoding = pos_encoding[tf.newaxis, ...]\n",
    "    return tf.cast(pos_encoding, tf.float32)\n",
    "\n",
    "  def call(self, inputs):\n",
    "    return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# 스케일드 닷 프로덕트 어텐션 함수\n",
    "def scaled_dot_product_attention(query, key, value, mask):\n",
    "  # 어텐션 가중치는 Q와 K의 닷 프로덕트\n",
    "  matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
    "\n",
    "  # 가중치를 정규화\n",
    "  depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "  logits = matmul_qk / tf.math.sqrt(depth)\n",
    "\n",
    "  # 패딩에 마스크 추가\n",
    "  if mask is not None:\n",
    "    logits += (mask * -1e9)\n",
    "\n",
    "  # softmax적용\n",
    "  attention_weights = tf.nn.softmax(logits, axis=-1)\n",
    "\n",
    "  # 최종 어텐션은 가중치와 V의 닷 프로덕트\n",
    "  output = tf.matmul(attention_weights, value)\n",
    "  return output\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "\n",
    "  def __init__(self, d_model, num_heads, name=\"multi_head_attention\"):\n",
    "    super(MultiHeadAttention, self).__init__(name=name)\n",
    "    self.num_heads = num_heads\n",
    "    self.d_model = d_model\n",
    "\n",
    "    assert d_model % self.num_heads == 0 # 나누어 떨어지지 않으면 수행 불가\n",
    "\n",
    "    self.depth = d_model // self.num_heads # 여기가 Q,K,V가 muti-head로 나뉘었을 때의 차원 수\n",
    "\n",
    "    self.query_dense = tf.keras.layers.Dense(units=d_model)\n",
    "    self.key_dense = tf.keras.layers.Dense(units=d_model)\n",
    "    self.value_dense = tf.keras.layers.Dense(units=d_model) # 아직 어텐션 계산 전 형태\n",
    "\n",
    "    self.dense = tf.keras.layers.Dense(units=d_model)# 다음 레이어로 넘어가는 최종 레이어 \n",
    "\n",
    "  def split_heads(self, inputs, batch_size):\n",
    "    inputs = tf.reshape(\n",
    "        inputs, shape=(batch_size, -1, self.num_heads, self.depth))# -1은 입력 문장의 길이\n",
    "    return tf.transpose(inputs, perm=[0, 2, 1, 3])# (batch_size, num_heads, seq_len, depth)\n",
    "\n",
    "  def call(self, inputs):\n",
    "    query, key, value, mask = inputs['query'], inputs['key'], inputs[\n",
    "        'value'], inputs['mask']\n",
    "    batch_size = tf.shape(query)[0]\n",
    "\n",
    "    # Q, K, V에 각각 Dense를 적용합니다\n",
    "    query = self.query_dense(query)\n",
    "    key = self.key_dense(key)\n",
    "    value = self.value_dense(value)\n",
    "\n",
    "    # 병렬 연산을 위한 머리를 여러 개 만듭니다\n",
    "    query = self.split_heads(query, tf.shape(query)[0])\n",
    "    key = self.split_heads(key, tf.shape(key)[0])\n",
    "    value = self.split_heads(value, tf.shape(value)[0])\n",
    "\n",
    "    # 스케일드 닷 프로덕트 어텐션 함수\n",
    "    scaled_attention = scaled_dot_product_attention(query, key, value, mask)\n",
    "\n",
    "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "\n",
    "    # 어텐션 연산 후에 각 결과를 다시 연결(concatenate)합니다\n",
    "    concat_attention = tf.reshape(scaled_attention,\n",
    "                                  (batch_size, -1, self.d_model))# (batch_size, seq_len, d_model)\n",
    "\n",
    "    # 최종 결과에도 Dense를 한 번 더 적용합니다\n",
    "    outputs = self.dense(concat_attention)\n",
    "\n",
    "    return outputs\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def create_padding_mask(x):\n",
    "  mask = tf.cast(tf.math.equal(x, 0), tf.float32)\n",
    "  # (batch_size, 1, 1, sequence length)\n",
    "  return mask[:, tf.newaxis, tf.newaxis , : ]\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def create_look_ahead_mask(x):\n",
    "  seq_len = tf.shape(x)[1]\n",
    "  look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "  padding_mask = create_padding_mask(x)\n",
    "  return tf.maximum(look_ahead_mask, padding_mask)\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# 인코더 하나의 레이어를 함수로 구현.\n",
    "# 이 하나의 레이어 안에는 두 개의 서브 레이어가 존재합니다.\n",
    "def encoder_layer(units, d_model, num_heads, dropout, name=\"encoder_layer\"):\n",
    "  inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "\n",
    "  # 패딩 마스크 사용\n",
    "  padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "\n",
    "  # 첫 번째 서브 레이어 : 멀티 헤드 어텐션 수행 (셀프 어텐션)\n",
    "  attention = MultiHeadAttention(\n",
    "      d_model, num_heads, name=\"attention\")({\n",
    "          'query': inputs,\n",
    "          'key': inputs,\n",
    "          'value': inputs,\n",
    "          'mask': padding_mask\n",
    "      })\n",
    "\n",
    "  # 어텐션의 결과는 Dropout과 Layer Normalization이라는 훈련을 돕는 테크닉을 수행\n",
    "  attention = tf.keras.layers.Dropout(rate=dropout)(attention)\n",
    "  attention = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(inputs + attention)\n",
    "\n",
    "  # 두 번째 서브 레이어 : 2개의 완전연결층\n",
    "  outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention)\n",
    "  outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "\n",
    "  # 완전연결층의 결과는 Dropout과 LayerNormalization이라는 훈련을 돕는 테크닉을 수행\n",
    "  outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "  outputs = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(attention + outputs)\n",
    "\n",
    "  return tf.keras.Model(\n",
    "      inputs=[inputs, padding_mask], outputs=outputs, name=name)\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def encoder(vocab_size,\n",
    "            num_layers,\n",
    "            units,\n",
    "            d_model,\n",
    "            num_heads,\n",
    "            dropout,\n",
    "            name=\"encoder\"):\n",
    "  inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "\n",
    "  # 패딩 마스크 사용\n",
    "  padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "\n",
    "  # 임베딩 레이어\n",
    "  embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "  embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "\n",
    "  # 포지셔널 인코딩\n",
    "  embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
    "\n",
    "  outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "\n",
    "  # num_layers만큼 쌓아올린 인코더의 층.\n",
    "  for i in range(num_layers):\n",
    "    outputs = encoder_layer(\n",
    "        units=units,\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        dropout=dropout,\n",
    "        name=\"encoder_layer_{}\".format(i),\n",
    "    )([outputs, padding_mask])\n",
    "\n",
    "  return tf.keras.Model(\n",
    "      inputs=[inputs, padding_mask], outputs=outputs, name=name)\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# 디코더 하나의 레이어를 함수로 구현.\n",
    "# 이 하나의 레이어 안에는 세 개의 서브 레이어가 존재합니다.\n",
    "def decoder_layer(units, d_model, num_heads, dropout, name=\"decoder_layer\"):\n",
    "  inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "  enc_outputs = tf.keras.Input(shape=(None, d_model), name=\"encoder_outputs\")\n",
    "  look_ahead_mask = tf.keras.Input(\n",
    "      shape=(1, None, None), name=\"look_ahead_mask\")\n",
    "  padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
    "\n",
    "  # 첫 번째 서브 레이어 : 멀티 헤드 어텐션 수행 (셀프 어텐션)\n",
    "  attention1 = MultiHeadAttention(\n",
    "      d_model, num_heads, name=\"attention_1\")(inputs={\n",
    "          'query': inputs,\n",
    "          'key': inputs,\n",
    "          'value': inputs,\n",
    "          'mask': look_ahead_mask\n",
    "      })\n",
    "\n",
    "  # 멀티 헤드 어텐션의 결과는 LayerNormalization이라는 훈련을 돕는 테크닉을 수행\n",
    "  attention1 = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(attention1 + inputs)\n",
    "\n",
    "  # 두 번째 서브 레이어 : 마스크드 멀티 헤드 어텐션 수행 (인코더-디코더 어텐션)\n",
    "  attention2 = MultiHeadAttention(\n",
    "      d_model, num_heads, name=\"attention_2\")(inputs={\n",
    "          'query': attention1,\n",
    "          'key': enc_outputs,\n",
    "          'value': enc_outputs,\n",
    "          'mask': padding_mask\n",
    "      })\n",
    "\n",
    "  # 마스크드 멀티 헤드 어텐션의 결과는\n",
    "  # Dropout과 LayerNormalization이라는 훈련을 돕는 테크닉을 수행\n",
    "  attention2 = tf.keras.layers.Dropout(rate=dropout)(attention2)\n",
    "  attention2 = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(attention2 + attention1)\n",
    "\n",
    "  # 세 번째 서브 레이어 : 2개의 완전연결층\n",
    "  outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention2)\n",
    "  outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "\n",
    "  # 완전연결층의 결과는 Dropout과 LayerNormalization 수행\n",
    "  outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "  outputs = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(outputs + attention2)\n",
    "\n",
    "  return tf.keras.Model(\n",
    "      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
    "      outputs=outputs,\n",
    "      name=name)\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def decoder(vocab_size,\n",
    "            num_layers,\n",
    "            units,\n",
    "            d_model,\n",
    "            num_heads,\n",
    "            dropout,\n",
    "            name='decoder'):\n",
    "  inputs = tf.keras.Input(shape=(None,), name='inputs')\n",
    "  enc_outputs = tf.keras.Input(shape=(None, d_model), name='encoder_outputs')\n",
    "  look_ahead_mask = tf.keras.Input(\n",
    "      shape=(1, None, None), name='look_ahead_mask')\n",
    "\n",
    "  # 패딩 마스크\n",
    "  padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
    "  \n",
    "  # 임베딩 레이어\n",
    "  embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "  embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "\n",
    "  # 포지셔널 인코딩\n",
    "  embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
    "\n",
    "  # Dropout이라는 훈련을 돕는 테크닉을 수행\n",
    "  outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "\n",
    "  for i in range(num_layers):\n",
    "    outputs = decoder_layer(\n",
    "        units=units,\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        dropout=dropout,\n",
    "        name='decoder_layer_{}'.format(i),\n",
    "    )(inputs=[outputs, enc_outputs, look_ahead_mask, padding_mask])\n",
    "\n",
    "  return tf.keras.Model(\n",
    "      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
    "      outputs=outputs,\n",
    "      name=name)\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def transformer(vocab_size,\n",
    "                num_layers,\n",
    "                units,\n",
    "                d_model,\n",
    "                num_heads,\n",
    "                dropout,\n",
    "                name=\"transformer\"):\n",
    "  inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "  dec_inputs = tf.keras.Input(shape=(None,), name=\"dec_inputs\")\n",
    "\n",
    "  # 인코더에서 패딩을 위한 마스크\n",
    "  enc_padding_mask = tf.keras.layers.Lambda(\n",
    "      create_padding_mask, output_shape=(1, 1, None),\n",
    "      name='enc_padding_mask')(inputs)\n",
    "\n",
    "  # 디코더에서 미래의 토큰을 마스크 하기 위해서 사용합니다.\n",
    "  # 내부적으로 패딩 마스크도 포함되어져 있습니다.\n",
    "  look_ahead_mask = tf.keras.layers.Lambda(\n",
    "      create_look_ahead_mask,\n",
    "      output_shape=(1, None, None),\n",
    "      name='look_ahead_mask')(dec_inputs)\n",
    "\n",
    "  # 두 번째 어텐션 블록에서 인코더의 벡터들을 마스킹\n",
    "  # 디코더에서 패딩을 위한 마스크\n",
    "  dec_padding_mask = tf.keras.layers.Lambda(\n",
    "      create_padding_mask, output_shape=(1, 1, None),\n",
    "      name='dec_padding_mask')(inputs)\n",
    "\n",
    "  # 인코더\n",
    "  enc_outputs = encoder(\n",
    "      vocab_size=vocab_size,\n",
    "      num_layers=num_layers,\n",
    "      units=units,\n",
    "      d_model=d_model,\n",
    "      num_heads=num_heads,\n",
    "      dropout=dropout,\n",
    "  )(inputs=[inputs, enc_padding_mask])\n",
    "\n",
    "  # 디코더\n",
    "  dec_outputs = decoder(\n",
    "      vocab_size=vocab_size,\n",
    "      num_layers=num_layers,\n",
    "      units=units,\n",
    "      d_model=d_model,\n",
    "      num_heads=num_heads,\n",
    "      dropout=dropout,\n",
    "  )(inputs=[dec_inputs, enc_outputs, look_ahead_mask, dec_padding_mask])\n",
    "\n",
    "  # 완전연결층\n",
    "  outputs = tf.keras.layers.Dense(units=vocab_size, name=\"outputs\")(dec_outputs)\n",
    "\n",
    "  return tf.keras.Model(inputs=[inputs, dec_inputs], outputs=outputs, name=name)\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " inputs (InputLayer)         [(None, None)]               0         []                            \n",
      "                                                                                                  \n",
      " dec_inputs (InputLayer)     [(None, None)]               0         []                            \n",
      "                                                                                                  \n",
      " enc_padding_mask (Lambda)   (None, 1, 1, None)           0         ['inputs[0][0]']              \n",
      "                                                                                                  \n",
      " encoder (Functional)        (None, None, 256)            3130368   ['inputs[0][0]',              \n",
      "                                                                     'enc_padding_mask[0][0]']    \n",
      "                                                                                                  \n",
      " look_ahead_mask (Lambda)    (None, 1, None, None)        0         ['dec_inputs[0][0]']          \n",
      "                                                                                                  \n",
      " dec_padding_mask (Lambda)   (None, 1, 1, None)           0         ['inputs[0][0]']              \n",
      "                                                                                                  \n",
      " decoder (Functional)        (None, None, 256)            3657728   ['dec_inputs[0][0]',          \n",
      "                                                                     'encoder[0][0]',             \n",
      "                                                                     'look_ahead_mask[0][0]',     \n",
      "                                                                     'dec_padding_mask[0][0]']    \n",
      "                                                                                                  \n",
      " outputs (Dense)             (None, None, 8110)           2084270   ['decoder[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 8872366 (33.85 MB)\n",
      "Trainable params: 8872366 (33.85 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# 하이퍼파라미터\n",
    "NUM_LAYERS = 2 # 인코더와 디코더의 층의 개수\n",
    "D_MODEL = 256 # 인코더와 디코더 내부의 입, 출력의 고정 차원\n",
    "NUM_HEADS = 8 # 멀티 헤드 어텐션에서의 헤드 수 \n",
    "UNITS = 512 # 피드 포워드 신경망의 은닉층의 크기\n",
    "DROPOUT = 0.1 # 드롭아웃의 비율\n",
    "\n",
    "model = transformer(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    units=UNITS,\n",
    "    d_model=D_MODEL,\n",
    "    num_heads=NUM_HEADS,\n",
    "    dropout=DROPOUT)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def loss_function(y_true, y_pred):\n",
    "  y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "  \n",
    "  loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "      from_logits=True, reduction='none')(y_true, y_pred)\n",
    "\n",
    "  mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n",
    "  loss = tf.multiply(loss, mask)\n",
    "\n",
    "  return tf.reduce_mean(loss)\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "\n",
    "  def __init__(self, d_model, warmup_steps=4000):\n",
    "    super(CustomSchedule, self).__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "    self.warmup_steps = warmup_steps\n",
    "\n",
    "  def __call__(self, step):\n",
    "    step = tf.cast(step, tf.float32)\n",
    "    arg1 = tf.math.rsqrt(step)\n",
    "    arg2 = step * (self.warmup_steps**-1.5)\n",
    "\n",
    "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "learning_rate = CustomSchedule(D_MODEL)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "  y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "  return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-14 13:23:36.405056: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:606] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2025-02-14 13:23:36.600641: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fdfe9774640 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2025-02-14 13:23:36.600689: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 3060, Compute Capability 8.6\n",
      "2025-02-14 13:23:36.610479: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-02-14 13:23:36.632646: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8600\n",
      "2025-02-14 13:23:36.704204: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2025-02-14 13:23:36.755409: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122/122 [==============================] - 27s 143ms/step - loss: 1.3747 - accuracy: 0.0183\n",
      "Epoch 2/100\n",
      "122/122 [==============================] - 11s 89ms/step - loss: 1.2532 - accuracy: 0.0256\n",
      "Epoch 3/100\n",
      "122/122 [==============================] - 8s 63ms/step - loss: 1.1411 - accuracy: 0.0256\n",
      "Epoch 4/100\n",
      "122/122 [==============================] - 8s 67ms/step - loss: 1.0769 - accuracy: 0.0258\n",
      "Epoch 5/100\n",
      "122/122 [==============================] - 8s 63ms/step - loss: 1.0265 - accuracy: 0.0278\n",
      "Epoch 6/100\n",
      "122/122 [==============================] - 7s 61ms/step - loss: 0.9831 - accuracy: 0.0307\n",
      "Epoch 7/100\n",
      "122/122 [==============================] - 8s 64ms/step - loss: 0.9417 - accuracy: 0.0330\n",
      "Epoch 8/100\n",
      "122/122 [==============================] - 8s 65ms/step - loss: 0.9002 - accuracy: 0.0355\n",
      "Epoch 9/100\n",
      "122/122 [==============================] - 7s 61ms/step - loss: 0.8548 - accuracy: 0.0387\n",
      "Epoch 10/100\n",
      "122/122 [==============================] - 7s 60ms/step - loss: 0.8052 - accuracy: 0.0431\n",
      "Epoch 11/100\n",
      "122/122 [==============================] - 7s 59ms/step - loss: 0.7500 - accuracy: 0.0481\n",
      "Epoch 12/100\n",
      "122/122 [==============================] - 8s 62ms/step - loss: 0.6903 - accuracy: 0.0537\n",
      "Epoch 13/100\n",
      "122/122 [==============================] - 8s 63ms/step - loss: 0.6282 - accuracy: 0.0604\n",
      "Epoch 14/100\n",
      "122/122 [==============================] - 8s 66ms/step - loss: 0.5626 - accuracy: 0.0676\n",
      "Epoch 15/100\n",
      "122/122 [==============================] - 8s 62ms/step - loss: 0.4955 - accuracy: 0.0759\n",
      "Epoch 16/100\n",
      "122/122 [==============================] - 8s 66ms/step - loss: 0.4258 - accuracy: 0.0859\n",
      "Epoch 17/100\n",
      "122/122 [==============================] - 8s 66ms/step - loss: 0.3567 - accuracy: 0.0968\n",
      "Epoch 18/100\n",
      "122/122 [==============================] - 8s 66ms/step - loss: 0.2893 - accuracy: 0.1086\n",
      "Epoch 19/100\n",
      "122/122 [==============================] - 7s 60ms/step - loss: 0.2267 - accuracy: 0.1205\n",
      "Epoch 20/100\n",
      "122/122 [==============================] - 7s 60ms/step - loss: 0.1698 - accuracy: 0.1314\n",
      "Epoch 21/100\n",
      "122/122 [==============================] - 7s 61ms/step - loss: 0.1226 - accuracy: 0.1405\n",
      "Epoch 22/100\n",
      "122/122 [==============================] - 7s 61ms/step - loss: 0.0869 - accuracy: 0.1463\n",
      "Epoch 23/100\n",
      "122/122 [==============================] - 7s 60ms/step - loss: 0.0636 - accuracy: 0.1491\n",
      "Epoch 24/100\n",
      "122/122 [==============================] - 7s 59ms/step - loss: 0.0501 - accuracy: 0.1506\n",
      "Epoch 25/100\n",
      "122/122 [==============================] - 8s 66ms/step - loss: 0.0410 - accuracy: 0.1516\n",
      "Epoch 26/100\n",
      "122/122 [==============================] - 8s 64ms/step - loss: 0.0370 - accuracy: 0.1516\n",
      "Epoch 27/100\n",
      "122/122 [==============================] - 7s 61ms/step - loss: 0.0343 - accuracy: 0.1519\n",
      "Epoch 28/100\n",
      "122/122 [==============================] - 7s 60ms/step - loss: 0.0323 - accuracy: 0.1521\n",
      "Epoch 29/100\n",
      "122/122 [==============================] - 7s 61ms/step - loss: 0.0303 - accuracy: 0.1523\n",
      "Epoch 30/100\n",
      "122/122 [==============================] - 8s 65ms/step - loss: 0.0297 - accuracy: 0.1523\n",
      "Epoch 31/100\n",
      "122/122 [==============================] - 8s 65ms/step - loss: 0.0290 - accuracy: 0.1523\n",
      "Epoch 32/100\n",
      "122/122 [==============================] - 7s 60ms/step - loss: 0.0281 - accuracy: 0.1524\n",
      "Epoch 33/100\n",
      "122/122 [==============================] - 7s 59ms/step - loss: 0.0282 - accuracy: 0.1524\n",
      "Epoch 34/100\n",
      "122/122 [==============================] - 7s 61ms/step - loss: 0.0277 - accuracy: 0.1523\n",
      "Epoch 35/100\n",
      "122/122 [==============================] - 8s 62ms/step - loss: 0.0245 - accuracy: 0.1531\n",
      "Epoch 36/100\n",
      "122/122 [==============================] - 7s 60ms/step - loss: 0.0229 - accuracy: 0.1534\n",
      "Epoch 37/100\n",
      "122/122 [==============================] - 7s 58ms/step - loss: 0.0207 - accuracy: 0.1541\n",
      "Epoch 38/100\n",
      "122/122 [==============================] - 7s 58ms/step - loss: 0.0190 - accuracy: 0.1544\n",
      "Epoch 39/100\n",
      "122/122 [==============================] - 7s 61ms/step - loss: 0.0174 - accuracy: 0.1550\n",
      "Epoch 40/100\n",
      "122/122 [==============================] - 8s 64ms/step - loss: 0.0169 - accuracy: 0.1551\n",
      "Epoch 41/100\n",
      "122/122 [==============================] - 8s 63ms/step - loss: 0.0160 - accuracy: 0.1552\n",
      "Epoch 42/100\n",
      "122/122 [==============================] - 7s 61ms/step - loss: 0.0146 - accuracy: 0.1556\n",
      "Epoch 43/100\n",
      "122/122 [==============================] - 7s 60ms/step - loss: 0.0138 - accuracy: 0.1557\n",
      "Epoch 44/100\n",
      "122/122 [==============================] - 7s 59ms/step - loss: 0.0128 - accuracy: 0.1560\n",
      "Epoch 45/100\n",
      "122/122 [==============================] - 8s 62ms/step - loss: 0.0120 - accuracy: 0.1561\n",
      "Epoch 46/100\n",
      "122/122 [==============================] - 8s 63ms/step - loss: 0.0116 - accuracy: 0.1564\n",
      "Epoch 47/100\n",
      "122/122 [==============================] - 7s 61ms/step - loss: 0.0106 - accuracy: 0.1565\n",
      "Epoch 48/100\n",
      "122/122 [==============================] - 7s 60ms/step - loss: 0.0109 - accuracy: 0.1565\n",
      "Epoch 49/100\n",
      "122/122 [==============================] - 7s 59ms/step - loss: 0.0102 - accuracy: 0.1567\n",
      "Epoch 50/100\n",
      "122/122 [==============================] - 7s 59ms/step - loss: 0.0093 - accuracy: 0.1569\n",
      "Epoch 51/100\n",
      "122/122 [==============================] - 7s 59ms/step - loss: 0.0094 - accuracy: 0.1569\n",
      "Epoch 52/100\n",
      "122/122 [==============================] - 7s 59ms/step - loss: 0.0088 - accuracy: 0.1570\n",
      "Epoch 53/100\n",
      "122/122 [==============================] - 7s 58ms/step - loss: 0.0081 - accuracy: 0.1572\n",
      "Epoch 54/100\n",
      "122/122 [==============================] - 7s 58ms/step - loss: 0.0082 - accuracy: 0.1571\n",
      "Epoch 55/100\n",
      "122/122 [==============================] - 7s 60ms/step - loss: 0.0081 - accuracy: 0.1571\n",
      "Epoch 56/100\n",
      "122/122 [==============================] - 7s 59ms/step - loss: 0.0073 - accuracy: 0.1573\n",
      "Epoch 57/100\n",
      "122/122 [==============================] - 7s 60ms/step - loss: 0.0068 - accuracy: 0.1575\n",
      "Epoch 58/100\n",
      "122/122 [==============================] - 8s 66ms/step - loss: 0.0076 - accuracy: 0.1573\n",
      "Epoch 59/100\n",
      "122/122 [==============================] - 8s 64ms/step - loss: 0.0067 - accuracy: 0.1576\n",
      "Epoch 60/100\n",
      "122/122 [==============================] - 7s 60ms/step - loss: 0.0064 - accuracy: 0.1576\n",
      "Epoch 61/100\n",
      "122/122 [==============================] - 7s 59ms/step - loss: 0.0065 - accuracy: 0.1575\n",
      "Epoch 62/100\n",
      "122/122 [==============================] - 7s 60ms/step - loss: 0.0063 - accuracy: 0.1576\n",
      "Epoch 63/100\n",
      "122/122 [==============================] - 7s 59ms/step - loss: 0.0059 - accuracy: 0.1577\n",
      "Epoch 64/100\n",
      "122/122 [==============================] - 7s 61ms/step - loss: 0.0057 - accuracy: 0.1577\n",
      "Epoch 65/100\n",
      "122/122 [==============================] - 8s 61ms/step - loss: 0.0061 - accuracy: 0.1577\n",
      "Epoch 66/100\n",
      "122/122 [==============================] - 7s 57ms/step - loss: 0.0055 - accuracy: 0.1577\n",
      "Epoch 67/100\n",
      "122/122 [==============================] - 7s 57ms/step - loss: 0.0054 - accuracy: 0.1578\n",
      "Epoch 68/100\n",
      "122/122 [==============================] - 7s 58ms/step - loss: 0.0047 - accuracy: 0.1579\n",
      "Epoch 69/100\n",
      "122/122 [==============================] - 7s 57ms/step - loss: 0.0051 - accuracy: 0.1579\n",
      "Epoch 70/100\n",
      "122/122 [==============================] - 7s 59ms/step - loss: 0.0049 - accuracy: 0.1579\n",
      "Epoch 71/100\n",
      "122/122 [==============================] - 7s 59ms/step - loss: 0.0046 - accuracy: 0.1580\n",
      "Epoch 72/100\n",
      "122/122 [==============================] - 8s 62ms/step - loss: 0.0048 - accuracy: 0.1579\n",
      "Epoch 73/100\n",
      "122/122 [==============================] - 8s 62ms/step - loss: 0.0043 - accuracy: 0.1581\n",
      "Epoch 74/100\n",
      "122/122 [==============================] - 8s 63ms/step - loss: 0.0045 - accuracy: 0.1580\n",
      "Epoch 75/100\n",
      "122/122 [==============================] - 7s 58ms/step - loss: 0.0041 - accuracy: 0.1580\n",
      "Epoch 76/100\n",
      "122/122 [==============================] - 7s 58ms/step - loss: 0.0040 - accuracy: 0.1580\n",
      "Epoch 77/100\n",
      "122/122 [==============================] - 7s 58ms/step - loss: 0.0044 - accuracy: 0.1580\n",
      "Epoch 78/100\n",
      "122/122 [==============================] - 7s 57ms/step - loss: 0.0040 - accuracy: 0.1581\n",
      "Epoch 79/100\n",
      "122/122 [==============================] - 7s 59ms/step - loss: 0.0038 - accuracy: 0.1581\n",
      "Epoch 80/100\n",
      "122/122 [==============================] - 8s 62ms/step - loss: 0.0038 - accuracy: 0.1581\n",
      "Epoch 81/100\n",
      "122/122 [==============================] - 7s 61ms/step - loss: 0.0039 - accuracy: 0.1580\n",
      "Epoch 82/100\n",
      "122/122 [==============================] - 7s 58ms/step - loss: 0.0039 - accuracy: 0.1580\n",
      "Epoch 83/100\n",
      "122/122 [==============================] - 7s 58ms/step - loss: 0.0037 - accuracy: 0.1581\n",
      "Epoch 84/100\n",
      "122/122 [==============================] - 7s 59ms/step - loss: 0.0035 - accuracy: 0.1581\n",
      "Epoch 85/100\n",
      "122/122 [==============================] - 7s 61ms/step - loss: 0.0037 - accuracy: 0.1582\n",
      "Epoch 86/100\n",
      "122/122 [==============================] - 7s 61ms/step - loss: 0.0035 - accuracy: 0.1581\n",
      "Epoch 87/100\n",
      "122/122 [==============================] - 7s 60ms/step - loss: 0.0036 - accuracy: 0.1581\n",
      "Epoch 88/100\n",
      "122/122 [==============================] - 8s 63ms/step - loss: 0.0032 - accuracy: 0.1582\n",
      "Epoch 89/100\n",
      "122/122 [==============================] - 7s 59ms/step - loss: 0.0033 - accuracy: 0.1582\n",
      "Epoch 90/100\n",
      "122/122 [==============================] - 7s 58ms/step - loss: 0.0031 - accuracy: 0.1582\n",
      "Epoch 91/100\n",
      "122/122 [==============================] - 7s 59ms/step - loss: 0.0031 - accuracy: 0.1583\n",
      "Epoch 92/100\n",
      "122/122 [==============================] - 7s 60ms/step - loss: 0.0029 - accuracy: 0.1582\n",
      "Epoch 93/100\n",
      "122/122 [==============================] - 7s 59ms/step - loss: 0.0029 - accuracy: 0.1582\n",
      "Epoch 94/100\n",
      "122/122 [==============================] - 7s 57ms/step - loss: 0.0027 - accuracy: 0.1583\n",
      "Epoch 95/100\n",
      "122/122 [==============================] - 7s 58ms/step - loss: 0.0027 - accuracy: 0.1583\n",
      "Epoch 96/100\n",
      "122/122 [==============================] - 7s 58ms/step - loss: 0.0027 - accuracy: 0.1583\n",
      "Epoch 97/100\n",
      "122/122 [==============================] - 7s 58ms/step - loss: 0.0029 - accuracy: 0.1583\n",
      "Epoch 98/100\n",
      "122/122 [==============================] - 7s 59ms/step - loss: 0.0026 - accuracy: 0.1583\n",
      "Epoch 99/100\n",
      "122/122 [==============================] - 7s 61ms/step - loss: 0.0027 - accuracy: 0.1583\n",
      "Epoch 100/100\n",
      "122/122 [==============================] - 8s 62ms/step - loss: 0.0028 - accuracy: 0.1583\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 100\n",
    "history = model.fit(dataset, epochs=EPOCHS, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5. 모델 평가하기\n",
    "\n",
    "Step 1에서 선택한 전처리 방법을 고려하여 입력된 문장에 대해서 대답을 얻는 예측 함수를 만듭니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def decoder_inference(sentence):\n",
    "  sentence = preprocess_korean_sentence(sentence)\n",
    "\n",
    "  # 입력된 문장을 정수 인코딩 후, 시작 토큰과 종료 토큰을 앞뒤로 추가.\n",
    "  # ex) Where have you been? → [[8331   86   30    5 1059    7 8332]]\n",
    "  sentence = tf.expand_dims(\n",
    "      START_TOKEN + tokenizer.encode(sentence) + END_TOKEN, axis=0)\n",
    "\n",
    "  # 디코더의 현재까지의 예측한 출력 시퀀스가 지속적으로 저장되는 변수.\n",
    "  # 처음에는 예측한 내용이 없음으로 시작 토큰만 별도 저장. ex) 8331\n",
    "  output_sequence = tf.expand_dims(START_TOKEN, 0)\n",
    "\n",
    "  # 디코더의 인퍼런스 단계\n",
    "  for i in range(MAX_LENGTH):\n",
    "    # 디코더는 최대 MAX_LENGTH의 길이만큼 다음 단어 예측을 반복합니다.\n",
    "    predictions = model(inputs=[sentence, output_sequence], training=False)\n",
    "    predictions = predictions[:, -1:, :]\n",
    "\n",
    "    # 현재 예측한 단어의 정수\n",
    "    predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "\n",
    "    # 만약 현재 예측한 단어가 종료 토큰이라면 for문을 종료\n",
    "    if tf.equal(predicted_id, END_TOKEN[0]):\n",
    "      break\n",
    "\n",
    "    # 예측한 단어들은 지속적으로 output_sequence에 추가됩니다.\n",
    "    # 이 output_sequence는 다시 디코더의 입력이 됩니다.\n",
    "    output_sequence = tf.concat([output_sequence, predicted_id], axis=-1)\n",
    "\n",
    "  return tf.squeeze(output_sequence, axis=0)\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def sentence_generation(sentence):\n",
    "  # 입력 문장에 대해서 디코더를 동작 시켜 예측된 정수 시퀀스를 리턴받습니다.\n",
    "  prediction = decoder_inference(sentence)\n",
    "\n",
    "  # 정수 시퀀스를 다시 텍스트 시퀀스로 변환합니다.\n",
    "  predicted_sentence = tokenizer.decode(\n",
    "      [i for i in prediction if i < tokenizer.vocab_size])\n",
    "\n",
    "  print('입력 : {}'.format(sentence))\n",
    "  print('출력 : {}'.format(predicted_sentence))\n",
    "\n",
    "  return predicted_sentence\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 최종 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 오늘 날씨 어때?\n",
      "출력 : 날씨 어플에 물어보세요\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'날씨 어플에 물어보세요'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation('오늘 날씨 어때?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 배가 너무 아파\n",
      "출력 : 좀 쉬세요\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'좀 쉬세요'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation('배가 너무 아파')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 회고\n",
    "1. accuracy가 0.1583으로 학습이 마무리 되었는데 최종 결과를 확인하면 성능이 너무 좋게 나와서 sparse categorical accuracy가 어떻게 측정되는지 알아보았다.\n",
    "    - y_pred는 model의 최종 출력으로 (batch_size, seq_len, vocab_size)이다.\n",
    "    - sparse categorical loss에 y_true와 y_pred가 매개변수로 들어가고 y_pred는 argmax가 적용된다고 한다.\n",
    "    - 그렇다면 accuracy는 예측 토큰 끼리의 정확도를 의미한다.\n",
    "    - 결론적으로 생성 모델에서 accuracy보다 정성평가가 더 중요하다는걸 정말 많이 느끼게 되었다.\n",
    "\n",
    "2. 전체적인 프로젝트 요구사항보다 먼저 한국어 텍스트 전처리에 대해 고민할 때, vocab 크기가 커지더라도 빈도 수를 고려해서 토큰을 줄이지 말아야겠다는 결론을 내렸었다. 또한 프로젝트의 요구사항에 따라 전처리를 따라가도 보니 어떤 토큰을 명시적으로 삭제하지 않고 vocab 크기를 먼저 결정짓는 방식의 토큰화를 진행하게 되었다. 해당 토큰화 방식이 어떻게 이루어지는지 더 자세히 살펴보았다.\n",
    "    - 서브워드 토큰화 원리\n",
    "    - 예. birdstrike => bird와 strike의 서브워드로 분리\n",
    "    - 주어진 말뭉치(corpus; 훈련데이터셋)에서 가장 자주 등장하는 서브워드 단위를 찾아 vocab를 구축한다.\n",
    "    - Greedy 토큰화; 문장을 토큰화할 때, 가장 긴 매칭 서브워드부터 시작하여 점진적으로 작은 단위로 분할한다.\n",
    "    - UNKOWN 처리; 어휘에 없는 단어는 더 작은 서브워드 단위로 분해된다. 개별 문자 단위까지도 분해할 수 있다.\n",
    "    - 형태학적으로 복잡한 언어나 신조어가 많은 도메인에 유용하다\n",
    "    - 결국 vocab을 구성하게 되는 방식은 항상 토큰의 등장 빈도수에 따라 결정되는 것 처럼 느껴진다. 과연 항상 그럴까?\n",
    "\n",
    "3. 이 내용은 Transformer 구조를 세세하게 뜯어 볼때 생겼던 의문 사항과 결론으로 다음에 헷갈리지 않기 위해 기록해놓으려 한다.\n",
    "    - tensor나 matrix 등 축이 3개 이상으로 많아질 때, Transpose를 적용하면 맨 마지막 2 축에만 적용된다.\n",
    "    - LSTM이나 RNN에서 유사한 태스크에 대해 학습할 때, 생각해보면 padding mask 라는걸 사용하지 않은 것을 알 수 있다. 왜냐하면 입력 sequence에서 패딩은 0으로 입력하기 때문에 복잡한 네트워크를 지나더라도 0이 출력되어 계산에 반영이 안되기 때문이다. 하지만 Transformer에서는 padding mask를 사용하는데 그 이유는 똑같이 0으로 출력되지만 attention_weight(어텐션 점수)를 계산할 때에는 Softmax 함수를 사용하기 때문에, 'e^0'은 '1'이 되어 패딩이 계산에 관여하게 되는 문제가 생긴다. 따라서 어떤 위치에 패딩이 들어갔는지 기억하고 해당 위치에 매우 큰 음수를 곱해서 0에 가깝게 만들어 줘야 한다. 이러한 이유로 padding mask가 필요하다.\n",
    "    - [포지셔널 인코딩](https://medium.com/thedeephub/positional-encoding-explained-a-deep-dive-into-transformer-pe-65cfe8cfe10b)에서 뒤로 갈수록 값의 변화가 없고 앞쪽으로 올수록 값의 변화가 잦은 이유는 pos/var 가 주기 함수의 변수로 들어가는데 뒤로 갈수록 var의 크기가 커져 주기가 늘어나는 효과가 나타나기 때문이다. 그럼 왜 이렇게 구현했을까? 논문의 저자는 \"모델이 상대적 위치에 따라 주의를 기울이는 법을 쉽게 학습할 수 있을 것\"이라는 가설을 세웠으며 이에 따라 고정된 오프셋k에 대해 PE(pos+k)는 PE(pos)의 선형 함수로 표현될 수 있다라고 하였다. 이 말은 PE(pos)가 PE(pos+K)로 이동할 수 있게 만드는 행렬 M이 존재하고 이 행렬 M은 pos와 독립적이라는 의미라고 한다. 즉, 시작하는 절대 위치와 관계없이 같은 방식(pos가 변화하여도)으로 작동한다는 뜻이다. 따라서 절대적인 위치 정보를 사용하지 않고도 상대적인 위치 정보를 학습할 수 있도록 한다는 의미이다. 예. 'not'이 문장에서 어디에 나타나는지에 관계 없이 근처 단어의 의미를 수정한다는 것을 학습할 수 있다는 의미."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
