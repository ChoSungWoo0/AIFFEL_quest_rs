{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipywidgets in /home/cho/anaconda3/envs/KoBert/lib/python3.8/site-packages (8.1.5)\n",
      "Requirement already satisfied: comm>=0.1.3 in /home/cho/anaconda3/envs/KoBert/lib/python3.8/site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /home/cho/anaconda3/envs/KoBert/lib/python3.8/site-packages (from ipywidgets) (7.33.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /home/cho/anaconda3/envs/KoBert/lib/python3.8/site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.12 in /home/cho/anaconda3/envs/KoBert/lib/python3.8/site-packages (from ipywidgets) (4.0.13)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.12 in /home/cho/anaconda3/envs/KoBert/lib/python3.8/site-packages (from ipywidgets) (3.0.13)\n",
      "Requirement already satisfied: setuptools>=18.5 in /home/cho/anaconda3/envs/KoBert/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (75.1.0)\n",
      "Requirement already satisfied: jedi>=0.16 in /home/cho/anaconda3/envs/KoBert/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.18.2)\n",
      "Requirement already satisfied: decorator in /home/cho/anaconda3/envs/KoBert/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: pickleshare in /home/cho/anaconda3/envs/KoBert/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /home/cho/anaconda3/envs/KoBert/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.43)\n",
      "Requirement already satisfied: pygments in /home/cho/anaconda3/envs/KoBert/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (2.18.0)\n",
      "Requirement already satisfied: backcall in /home/cho/anaconda3/envs/KoBert/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: matplotlib-inline in /home/cho/anaconda3/envs/KoBert/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: pexpect>4.3 in /home/cho/anaconda3/envs/KoBert/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /home/cho/anaconda3/envs/KoBert/lib/python3.8/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/cho/anaconda3/envs/KoBert/lib/python3.8/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /home/cho/anaconda3/envs/KoBert/lib/python3.8/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Collecting git+https://****@github.com/SKTBrain/KoBERT.git@master\n",
      "  Cloning https://****@github.com/SKTBrain/KoBERT.git (to revision master) to /tmp/pip-req-build-06lmazbg\n",
      "  Running command git clone --filter=blob:none --quiet 'https://****@github.com/SKTBrain/KoBERT.git' /tmp/pip-req-build-06lmazbg\n",
      "  Resolved https://****@github.com/SKTBrain/KoBERT.git to commit 5c46b1c68e4755b54879431bd302db621f4d2f47\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: boto3<=1.15.18 in /home/cho/anaconda3/envs/KoBert/lib/python3.8/site-packages (from kobert==0.2.3) (1.15.18)\n",
      "Requirement already satisfied: gluonnlp<=0.10.0,>=0.6.0 in /home/cho/anaconda3/envs/KoBert/lib/python3.8/site-packages (from kobert==0.2.3) (0.10.0)\n",
      "Requirement already satisfied: mxnet<=1.7.0.post2,>=1.4.0 in /home/cho/anaconda3/envs/KoBert/lib/python3.8/site-packages (from kobert==0.2.3) (1.7.0.post2)\n",
      "Requirement already satisfied: onnxruntime<=1.8.0,==1.8.0 in /home/cho/anaconda3/envs/KoBert/lib/python3.8/site-packages (from kobert==0.2.3) (1.8.0)\n",
      "Requirement already satisfied: sentencepiece<=0.1.96,>=0.1.6 in /home/cho/anaconda3/envs/KoBert/lib/python3.8/site-packages (from kobert==0.2.3) (0.1.96)\n",
      "Collecting torch<=1.10.1,>=1.7.0 (from kobert==0.2.3)\n",
      "  Using cached torch-1.10.1-cp38-cp38-manylinux1_x86_64.whl.metadata (24 kB)\n",
      "Requirement already satisfied: transformers<=4.8.1,>=4.8.1 in /home/cho/anaconda3/envs/KoBert/lib/python3.8/site-packages (from kobert==0.2.3) (4.8.1)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /home/cho/anaconda3/envs/KoBert/lib/python3.8/site-packages (from onnxruntime<=1.8.0,==1.8.0->kobert==0.2.3) (1.23.1)\n",
      "Requirement already satisfied: protobuf in /home/cho/anaconda3/envs/KoBert/lib/python3.8/site-packages (from onnxruntime<=1.8.0,==1.8.0->kobert==0.2.3) (5.29.3)\n",
      "Requirement already satisfied: flatbuffers in /home/cho/anaconda3/envs/KoBert/lib/python3.8/site-packages (from onnxruntime<=1.8.0,==1.8.0->kobert==0.2.3) (25.2.10)\n",
      "Requirement already satisfied: botocore<1.19.0,>=1.18.18 in /home/cho/anaconda3/envs/KoBert/lib/python3.8/site-packages (from boto3<=1.15.18->kobert==0.2.3) (1.18.18)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/cho/anaconda3/envs/KoBert/lib/python3.8/site-packages (from boto3<=1.15.18->kobert==0.2.3) (0.10.0)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /home/cho/anaconda3/envs/KoBert/lib/python3.8/site-packages (from boto3<=1.15.18->kobert==0.2.3) (0.3.7)\n",
      "Requirement already satisfied: cython in /home/cho/anaconda3/envs/KoBert/lib/python3.8/site-packages (from gluonnlp<=0.10.0,>=0.6.0->kobert==0.2.3) (3.0.12)\n",
      "Requirement already satisfied: packaging in /home/cho/anaconda3/envs/KoBert/lib/python3.8/site-packages (from gluonnlp<=0.10.0,>=0.6.0->kobert==0.2.3) (24.2)\n",
      "Requirement already satisfied: requests<3,>=2.20.0 in /home/cho/anaconda3/envs/KoBert/lib/python3.8/site-packages (from mxnet<=1.7.0.post2,>=1.4.0->kobert==0.2.3) (2.32.3)\n",
      "Requirement already satisfied: graphviz<0.9.0,>=0.8.1 in /home/cho/anaconda3/envs/KoBert/lib/python3.8/site-packages (from mxnet<=1.7.0.post2,>=1.4.0->kobert==0.2.3) (0.8.4)\n",
      "Requirement already satisfied: typing-extensions in /home/cho/anaconda3/envs/KoBert/lib/python3.8/site-packages (from torch<=1.10.1,>=1.7.0->kobert==0.2.3) (4.11.0)\n",
      "Requirement already satisfied: filelock in /home/cho/anaconda3/envs/KoBert/lib/python3.8/site-packages (from transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub==0.0.12 in /home/cho/anaconda3/envs/KoBert/lib/python3.8/site-packages (from transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (0.0.12)\n",
      "Requirement already satisfied: pyyaml in /home/cho/anaconda3/envs/KoBert/lib/python3.8/site-packages (from transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/cho/anaconda3/envs/KoBert/lib/python3.8/site-packages (from transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (2024.11.6)\n",
      "Requirement already satisfied: sacremoses in /home/cho/anaconda3/envs/KoBert/lib/python3.8/site-packages (from transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (0.1.1)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /home/cho/anaconda3/envs/KoBert/lib/python3.8/site-packages (from transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (0.10.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/cho/anaconda3/envs/KoBert/lib/python3.8/site-packages (from transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (4.67.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/cho/anaconda3/envs/KoBert/lib/python3.8/site-packages (from botocore<1.19.0,>=1.18.18->boto3<=1.15.18->kobert==0.2.3) (2.8.2)\n",
      "Requirement already satisfied: urllib3<1.26,>=1.20 in /home/cho/anaconda3/envs/KoBert/lib/python3.8/site-packages (from botocore<1.19.0,>=1.18.18->boto3<=1.15.18->kobert==0.2.3) (1.25.11)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/cho/anaconda3/envs/KoBert/lib/python3.8/site-packages (from requests<3,>=2.20.0->mxnet<=1.7.0.post2,>=1.4.0->kobert==0.2.3) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/cho/anaconda3/envs/KoBert/lib/python3.8/site-packages (from requests<3,>=2.20.0->mxnet<=1.7.0.post2,>=1.4.0->kobert==0.2.3) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/cho/anaconda3/envs/KoBert/lib/python3.8/site-packages (from requests<3,>=2.20.0->mxnet<=1.7.0.post2,>=1.4.0->kobert==0.2.3) (2025.1.31)\n",
      "Requirement already satisfied: click in /home/cho/anaconda3/envs/KoBert/lib/python3.8/site-packages (from sacremoses->transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (8.1.8)\n",
      "Requirement already satisfied: joblib in /home/cho/anaconda3/envs/KoBert/lib/python3.8/site-packages (from sacremoses->transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (1.4.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/cho/anaconda3/envs/KoBert/lib/python3.8/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.19.0,>=1.18.18->boto3<=1.15.18->kobert==0.2.3) (1.16.0)\n",
      "Using cached torch-1.10.1-cp38-cp38-manylinux1_x86_64.whl (881.9 MB)\n",
      "Installing collected packages: torch\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.11.0+cu113\n",
      "    Uninstalling torch-1.11.0+cu113:\n",
      "      Successfully uninstalled torch-1.11.0+cu113\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchvision 0.12.0+cu113 requires torch==1.11.0, but you have torch 1.10.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed torch-1.10.1\n"
     ]
    }
   ],
   "source": [
    "!pip install ipywidgets  # for vscode\n",
    "!pip install git+https://git@github.com/SKTBrain/KoBERT.git@master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import gluonnlp as nlp\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kobert import get_tokenizer\n",
    "from kobert import get_pytorch_kobert_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model. /home/cho/test/.cache/kobert_v1.zip\n",
      "using cached model. /home/cho/test/.cache/kobert_news_wiki_ko_cased-1087f8699e.spiece\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cho/anaconda3/envs/KoBert/lib/python3.8/site-packages/transformers/modeling_utils.py:1211: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(resolved_archive_file, map_location=\"cpu\")\n"
     ]
    }
   ],
   "source": [
    "bertmodel, vocab = get_pytorch_kobert_model(cachedir=\".cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       지금 너 스스로를 죽여달라고 애원하는 것인가?  아닙니다. 죄송합니다.  죽을 거면...\n",
      "1       길동경찰서입니다. 9시 40분 마트에 폭발물을 설치할거다. 네? 똑바로 들어 한번만...\n",
      "2       너 되게 귀여운거 알지? 나보다 작은 남자는 첨봤어. 그만해. 니들 놀리는거 재미없...\n",
      "3       어이 거기 예?? 너 말이야 너. 이리 오라고 무슨 일. 너 옷 좋아보인다? 얘 돈...\n",
      "4       저기요 혹시 날이 너무 뜨겁잖아요? 저희 회사에서 이 선크림 파는데 한 번 손등에 ...\n",
      "                              ...                        \n",
      "4841    요즘 나는 축구 배워보고 싶다 키키 축구? 축구 룰에 대해서는 알아? 오오 알지 키...\n",
      "4842    프로게이머들은 좋아하는 거 하고 돈 버니까 좋겠다 키키 부럽. 임요한 진짜 돈 많이...\n",
      "4843    너는 해외여행 자주 다니는 편이야? 키키 아니. 비행기 타는 걸 안 좋아해서 해외여...\n",
      "4844    겨울에 눈이 제발 적게 오면 좋겠다 나도 키키 어릴 땐 눈 오는 게 되게 좋아했었는...\n",
      "4845    요즘은 비나 태풍이 한번 와도 피해가 큰 듯 어 ㅠㅠ 나는 아직 태풍 매미를 잊지 ...\n",
      "Name: conversation, Length: 4846, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from io import StringIO\n",
    "\n",
    "train = pd.read_csv('/home/cho/AIFFEL_quest_rs/GoingDeeper/Go01/raw_data.csv')\n",
    "tsv_train = train.to_csv(sep='\\t', index=False)\n",
    "\n",
    "tsv_df = pd.read_csv(StringIO(tsv_train), sep=\"\\t\")\n",
    "# \\n을 공백으로 바꿔서 conversation 컬럼 처리\n",
    "tsv_df['conversation'] = tsv_df['conversation'].replace(r'\\n', ' ', regex=True)\n",
    "print(tsv_df['conversation'])\n",
    "\n",
    "# 수정된 데이터를 파일로 저장\n",
    "tsv_df.to_csv('train.tsv', sep='\\t', index=False)\n",
    "\n",
    "\n",
    "dataset_train = nlp.data.TSVDataset('./train.tsv',field_indices=[2,3], num_discard_samples=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       지금 너 스스로를 죽여달라고 애원하는 것인가?  아닙니다. 죄송합니다.  죽을 거면...\n",
      "1       길동경찰서입니다. 9시 40분 마트에 폭발물을 설치할거다. 네? 똑바로 들어 한번만...\n",
      "2       너 되게 귀여운거 알지? 나보다 작은 남자는 첨봤어. 그만해. 니들 놀리는거 재미없...\n",
      "3       어이 거기 예?? 너 말이야 너. 이리 오라고 무슨 일. 너 옷 좋아보인다? 얘 돈...\n",
      "4       저기요 혹시 날이 너무 뜨겁잖아요? 저희 회사에서 이 선크림 파는데 한 번 손등에 ...\n",
      "                              ...                        \n",
      "4841    요즘 나는 축구 배워보고 싶다 키키 축구? 축구 룰에 대해서는 알아? 오오 알지 키...\n",
      "4842    프로게이머들은 좋아하는 거 하고 돈 버니까 좋겠다 키키 부럽. 임요한 진짜 돈 많이...\n",
      "4843    너는 해외여행 자주 다니는 편이야? 키키 아니. 비행기 타는 걸 안 좋아해서 해외여...\n",
      "4844    겨울에 눈이 제발 적게 오면 좋겠다 나도 키키 어릴 땐 눈 오는 게 되게 좋아했었는...\n",
      "4845    요즘은 비나 태풍이 한번 와도 피해가 큰 듯 어 ㅠㅠ 나는 아직 태풍 매미를 잊지 ...\n",
      "Name: conversation, Length: 4846, dtype: object\n"
     ]
    }
   ],
   "source": [
    "test = pd.read_csv('/home/cho/AIFFEL_quest_rs/GoingDeeper/Go01/augmented_replaced_text_data.csv')\n",
    "tsv_test = train.to_csv(sep='\\t', index=False)\n",
    "\n",
    "tsv_df = pd.read_csv(StringIO(tsv_test), sep=\"\\t\")\n",
    "# \\n을 공백으로 바꿔서 conversation 컬럼 처리\n",
    "tsv_df['conversation'] = tsv_df['conversation'].replace(r'\\n', ' ', regex=True)\n",
    "print(tsv_df['conversation'])\n",
    "\n",
    "# 수정된 데이터를 파일로 저장\n",
    "tsv_df.to_csv('test.tsv', sep='\\t', index=False)\n",
    "\n",
    "\n",
    "dataset_test = nlp.data.TSVDataset('./test.tsv',field_indices=[2,3], num_discard_samples=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, max_len,\n",
    "                 pad, pair):\n",
    "        transform = nlp.data.BERTSentenceTransform(\n",
    "            bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)\n",
    "\n",
    "        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n",
    "        self.labels = [np.int32(i[label_idx]) for i in dataset]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return (self.sentences[i] + (self.labels[i], ))\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model. /home/cho/test/.cache/kobert_news_wiki_ko_cased-1087f8699e.spiece\n"
     ]
    }
   ],
   "source": [
    "tokenizer = get_tokenizer()\n",
    "tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting parameters\n",
    "max_len = 64\n",
    "batch_size = 64\n",
    "warmup_ratio = 0.1\n",
    "num_epochs = 5\n",
    "max_grad_norm = 1\n",
    "log_interval = 200\n",
    "learning_rate =  5e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = BERTDataset(dataset_train, 0, 1, tok, max_len, True, False)\n",
    "data_test = BERTDataset(dataset_test, 0, 1, tok, max_len, True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(data_train, batch_size=batch_size, num_workers=5)\n",
    "test_dataloader = torch.utils.data.DataLoader(data_test, batch_size=batch_size, num_workers=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 hidden_size = 768,\n",
    "                 num_classes=5,\n",
    "                 dr_rate=None,\n",
    "                 params=None):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.dr_rate = dr_rate\n",
    "                 \n",
    "        self.classifier = nn.Linear(hidden_size , num_classes)\n",
    "        if dr_rate:\n",
    "            self.dropout = nn.Dropout(p=dr_rate)\n",
    "    \n",
    "    def gen_attention_mask(self, token_ids, valid_length):\n",
    "        attention_mask = torch.zeros_like(token_ids)\n",
    "        for i, v in enumerate(valid_length):\n",
    "            attention_mask[i][:v] = 1\n",
    "        return attention_mask.float()\n",
    "\n",
    "    def forward(self, token_ids, valid_length, segment_ids):\n",
    "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
    "        \n",
    "        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n",
    "        if self.dr_rate:\n",
    "            out = self.dropout(pooler)\n",
    "        else:\n",
    "            out = pooler\n",
    "        return self.classifier(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BERTClassifier(bertmodel,  dr_rate=0.5).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare optimizer and schedule (linear warmup and decay)\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_total = len(train_dataloader) * num_epochs\n",
    "warmup_step = int(t_total * warmup_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy(X,Y):\n",
    "    max_vals, max_indices = torch.max(X, 1)\n",
    "    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
    "    return train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81be079ddd2345278c9a8f27e6c536e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/76 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 batch id 1 loss 1.7605568170547485 train acc 0.125\n",
      "epoch 1 train acc 0.4146792763157895\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59dfda3f831f41b6bb1ab5b8f89416db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/76 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 test acc 0.20929276315789475\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1b478ebd3a946b790e8f31f9e3faabc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/76 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 batch id 1 loss 4.096278667449951 train acc 0.0\n",
      "epoch 2 train acc 0.4261924342105263\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24764a520b4d488c8d5231196dae3c21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/76 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 test acc 0.2099095394736842\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77b3194cd9b24497b1d6ee47fdf9389d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/76 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 batch id 1 loss 4.517693996429443 train acc 0.0\n",
      "epoch 3 train acc 0.4198190789473684\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e90e0e7334ff475693beb5ef8b5ea8f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/76 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 test acc 0.2236842105263158\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b25cef05016447da964c5b980dc77d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/76 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 batch id 1 loss 4.842550277709961 train acc 0.0\n",
      "epoch 4 train acc 0.4251287185354691\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93925dfcdad34cd8a1571714c4fa9cec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/76 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 test acc 0.5929723255148742\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "432fbf31df3b4746b434a86796dcfe23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/76 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 batch id 1 loss 1.7423129081726074 train acc 0.28125\n",
      "epoch 5 train acc 0.5172250429061784\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a38a59087ed64b0ebef70a8cc96cc5c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/76 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 test acc 0.6083470394736842\n"
     ]
    }
   ],
   "source": [
    "for e in range(num_epochs):\n",
    "    train_acc = 0.0\n",
    "    test_acc = 0.0\n",
    "    model.train()\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in tqdm(enumerate(train_dataloader), total=len(train_dataloader)):\n",
    "        optimizer.zero_grad()\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        loss = loss_fn(out, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        optimizer.step()\n",
    "        scheduler.step()  # Update learning rate schedule\n",
    "        train_acc += calc_accuracy(out, label)\n",
    "        if batch_id % log_interval == 0:\n",
    "            print(\"epoch {} batch id {} loss {} train acc {}\".format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc / (batch_id+1)))\n",
    "    print(\"epoch {} train acc {}\".format(e+1, train_acc / (batch_id+1)))\n",
    "    model.eval()\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in tqdm(enumerate(test_dataloader), total=len(test_dataloader)):\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        test_acc += calc_accuracy(out, label)\n",
    "    print(\"epoch {} test acc {}\".format(e+1, test_acc / (batch_id+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측된 클래스: 3\n"
     ]
    }
   ],
   "source": [
    "def inference(model, tokenizer, vocab, sentence, device, max_len=64):\n",
    "    model.eval()  # 평가 모드 설정\n",
    "\n",
    "    # 문장 토큰화\n",
    "    tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)\n",
    "    transform = nlp.data.BERTSentenceTransform(tok, max_seq_length=max_len, pad=True, pair=False)\n",
    "    \n",
    "    # 토큰 변환\n",
    "    tokenized_sentence = transform([sentence])\n",
    "    token_ids = torch.tensor(tokenized_sentence[0]).unsqueeze(0).long().to(device)\n",
    "    valid_length = torch.tensor(tokenized_sentence[1]).unsqueeze(0).long().to(device)\n",
    "    segment_ids = torch.tensor(tokenized_sentence[2]).unsqueeze(0).long().to(device)\n",
    "\n",
    "    # 모델 예측\n",
    "    with torch.no_grad():\n",
    "        output = model(token_ids, valid_length, segment_ids)\n",
    "        predicted_class = torch.argmax(output, dim=1).cpu().numpy()[0]\n",
    "\n",
    "    return predicted_class\n",
    "\n",
    "\n",
    "sentence = '너가 민수라는 남자 제자니? 네 맞는데요 왜 그러시죠? 혹시? 무슨 일은 무슨 일은 그냥 선생님 콩팥팥 정도 가져가려고 네.? 장난치시지 마세요. 장난이 너무 심하신 거 아니신가요? 너무 어린애라 그런지 상황 판단 못하네. 보여 줄 테니 봐봐 장난인지 아닌지 어.? 왜 그러시는 건가요 신고 할 건데 당장 신고 할 거예요. 신고해보던지 어딘지는 알고 신고하게? 저한테 물어볼 거 있으시면 최대한 할게요 선생님 해치지 말아주세요. 내가 말하는 거 보여 주기만 할게요 그러면 그냥 깔끔하게 끝날 일이야 그렇게 바로 할게요 바로 할게요'\n",
    "\n",
    "predicted_label = inference(model, tokenizer, vocab, sentence, device)\n",
    "\n",
    "# 예제 문장 예측\n",
    "print(f\"예측된 클래스: {predicted_label}\") #협박 대화 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "KoBert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
